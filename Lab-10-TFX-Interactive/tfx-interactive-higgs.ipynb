{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wdeKOEkv1Fe8"
   },
   "source": [
    "##### Copyright &copy; 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c2jyGuiG1gHr"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "23R0Z9RojXYW"
   },
   "source": [
    "# TFX interactive development.\n",
    "\n",
    "This notebook demonstrates how to use Jupyter notebooks for TFX iterative development.  \n",
    "\n",
    "Working in an interactive notebook is a useful way to become familiar with the structure of a TFX pipeline.  It's also useful when doing development of your own pipelines as a lightweight development environment, but you should be aware that there are differences in the way interactive notebooks are orchestrated, and how they access metadata artifacts.\n",
    "\n",
    "## Orchestration\n",
    "\n",
    "In a production deployment of TFX you will use an orchestrator such as Apache Airflow, Kubeflow, or Apache Beam.  In an interactive notebook the notebook itself is the orchestrator, running each TFX component as you execute the notebook cells.\n",
    "\n",
    "## Metadata\n",
    "\n",
    "In a production deployment of TFX you will access metadata through the ML Metadata (MLMD) API.  MLMD stores metadata properties in a database such as MySQL, and stores the metadata payloads in a persistent store such as on your filesystem.  In an interactive notebook, both properties and payloads are stored in the /tmp directory on the Jupyter notebook or Colab server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2GivNBNYjb3b"
   },
   "source": [
    "## Setup\n",
    "First, install the necessary packages, download data, import modules and set up paths.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NMzm7YerqwBS"
   },
   "source": [
    "### Set Colab to TFX 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qGLQhxjKq3EJ"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  %tensorflow_version 2.x\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tbWskRN2qsCD"
   },
   "source": [
    "### Install TFX\n",
    "\n",
    "> #### Note\n",
    "> Because of some of the updates to packages you must use the button at the bottom of the output of this cell to restart the runtime.  Following restart, you should rerun this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bRbSCw-U-h_F"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U \\\n",
    "  tfx==0.15.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N-ePgV0Lj68Q"
   },
   "source": [
    "### Import packages\n",
    "Import necessary packages, including standard TFX component classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YIqpWK9efviJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "import urllib\n",
    "\n",
    "import tensorflow as tf\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "import tfx\n",
    "from tfx.components.evaluator.component import Evaluator\n",
    "from tfx.components.example_gen.csv_example_gen.component import CsvExampleGen\n",
    "from tfx.components.example_validator.component import ExampleValidator\n",
    "from tfx.components.model_validator.component import ModelValidator\n",
    "from tfx.components.pusher.component import Pusher\n",
    "from tfx.components.schema_gen.component import SchemaGen\n",
    "from tfx.components.statistics_gen.component import StatisticsGen\n",
    "from tfx.components.trainer.component import Trainer\n",
    "from tfx.components.transform.component import Transform\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
    "from tfx.proto import evaluator_pb2\n",
    "from tfx.proto import pusher_pb2\n",
    "from tfx.proto import trainer_pb2\n",
    "from tfx.utils.dsl_utils import external_input\n",
    "\n",
    "from tensorflow.core.example import example_pb2\n",
    "from tensorflow_metadata.proto.v0 import anomalies_pb2\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "from tensorflow_metadata.proto.v0 import statistics_pb2\n",
    "\n",
    "import tensorflow_data_validation as tfdv\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow_model_analysis as tfma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G51uXGCsPlj8"
   },
   "source": [
    "Check the versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XZY7Pnoxmoe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.15.0\n",
      "TFX version: 0.15.0rc0\n",
      "TFT version: 0.15.0\n",
      "TFDV version: 0.15.0\n",
      "TFMA version: 0.15.4\n"
     ]
    }
   ],
   "source": [
    "print('TensorFlow version: {}'.format(tf.__version__))\n",
    "print('TFX version: {}'.format(tfx.__version__))\n",
    "print('TFT version: {}'.format(tft.__version__))\n",
    "print('TFDV version: {}'.format(tfdv.__version__))\n",
    "print('TFMA version: {}'.format(tfma.VERSION_STRING))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n2cMMAbSkGfX"
   },
   "source": [
    "### Download example data\n",
    "Download the sample dataset for use in our TFX pipeline.\n",
    "\n",
    "The data has been produced using Monte Carlo simulations. The first 21 features (columns 2-22) are kinematic properties measured by the particle detectors in the accelerator. The last seven features are functions of the first 21 features; these are high-level features derived by physicists to help discriminate between the two classes.\n",
    "\n",
    "The first column is the class label (1 for signal, 0 for background), followed by the 28 features (21 low-level features then 7 high-level features): lepton pT, lepton eta, lepton phi, missing energy magnitude, missing energy phi, jet 1 pt, jet 1 eta, jet 1 phi, jet 1 b-tag, jet 2 pt, jet 2 eta, jet 2 phi, jet 2 b-tag, jet 3 pt, jet 3 eta, jet 3 phi, jet 3 b-tag, jet 4 pt, jet 4 eta, jet 4 phi, jet 4 b-tag, m_jj, m_jjj, m_lv, m_jlv, m_bb, m_wbb, m_wwbb.\n",
    "\n",
    "The data comes from the UCI Machine Learning Repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/tmp/tfx-data59as85lu/data.csv', <http.client.HTTPMessage at 0x7f4b1f378198>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_data_root = tempfile.mkdtemp(prefix='tfx-data')\n",
    "DATA_PATH = 'https://storage.googleapis.com/workshop-datasets/higgs/tiny/HIGGS.csv'\n",
    "_data_filepath = os.path.join(_data_root, \"data.csv\")\n",
    "urllib.request.urlretrieve(DATA_PATH, _data_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r2vaxwUvQRzO"
   },
   "source": [
    "Take a quick look at the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hqn4wST2Bex5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_label,lepton_pT,lepton_eta,lepton_phi,missing_energy_magnitude,missing_energy_phi,jet_1_pt,jet_1_eta,jet_1_phi,jet_1_b_tag,jet_2_pt,jet_2_eta,jet_2_phi,jet_2_b_tag,jet_3_pt,jet_3_eta,jet_3_phi,jet_3_b_tag,jet_4_pt,jet_4_eta,jet_4_phi,jet_4_b_tag,m_jj,m_jjj,m_lv,m_jlv,m_bb,m_wbb,m_wwbb\n",
      "1.000000000000000000e+00,8.692932128906250000e-01,-6.350818276405334473e-01,2.256902605295181274e-01,3.274700641632080078e-01,-6.899932026863098145e-01,7.542022466659545898e-01,-2.485731393098831177e-01,-1.092063903808593750e+00,0.000000000000000000e+00,1.374992132186889648e+00,-6.536741852760314941e-01,9.303491115570068359e-01,1.107436060905456543e+00,1.138904333114624023e+00,-1.578198313713073730e+00,-1.046985387802124023e+00,0.000000000000000000e+00,6.579295396804809570e-01,-1.045456994324922562e-02,-4.576716944575309753e-02,3.101961374282836914e+00,1.353760004043579102e+00,9.795631170272827148e-01,9.780761599540710449e-01,9.200048446655273438e-01,7.216574549674987793e-01,9.887509346008300781e-01,8.766783475875854492e-01\n",
      "1.000000000000000000e+00,9.075421094894409180e-01,3.291472792625427246e-01,3.594118654727935791e-01,1.497969865798950195e+00,-3.130095303058624268e-01,1.095530629158020020e+00,-5.575249195098876953e-01,-1.588229775428771973e+00,2.173076152801513672e+00,8.125811815261840820e-01,-2.136419266462326050e-01,1.271014571189880371e+00,2.214872121810913086e+00,4.999939501285552979e-01,-1.261431813240051270e+00,7.321561574935913086e-01,0.000000000000000000e+00,3.987008929252624512e-01,-1.138930082321166992e+00,-8.191101951524615288e-04,0.000000000000000000e+00,3.022198975086212158e-01,8.330481648445129395e-01,9.856996536254882812e-01,9.780983924865722656e-01,7.797321677207946777e-01,9.923557639122009277e-01,7.983425855636596680e-01\n",
      "1.000000000000000000e+00,7.988347411155700684e-01,1.470638751983642578e+00,-1.635974764823913574e+00,4.537731707096099854e-01,4.256291687488555908e-01,1.104874610900878906e+00,1.282322287559509277e+00,1.381664276123046875e+00,0.000000000000000000e+00,8.517372012138366699e-01,1.540658950805664062e+00,-8.196895122528076172e-01,2.214872121810913086e+00,9.934899210929870605e-01,3.560801148414611816e-01,-2.087775468826293945e-01,2.548224449157714844e+00,1.256954550743103027e+00,1.128847599029541016e+00,9.004608392715454102e-01,0.000000000000000000e+00,9.097532629966735840e-01,1.108330488204956055e+00,9.856922030448913574e-01,9.513312578201293945e-01,8.032515048980712891e-01,8.659244179725646973e-01,7.801175713539123535e-01\n",
      "0.000000000000000000e+00,1.344384789466857910e+00,-8.766260147094726562e-01,9.359127283096313477e-01,1.992050051689147949e+00,8.824543952941894531e-01,1.786065936088562012e+00,-1.646777749061584473e+00,-9.423825144767761230e-01,0.000000000000000000e+00,2.423264741897583008e+00,-6.760157942771911621e-01,7.361586689949035645e-01,2.214872121810913086e+00,1.298719763755798340e+00,-1.430738091468811035e+00,-3.646581768989562988e-01,0.000000000000000000e+00,7.453126907348632812e-01,-6.783788204193115234e-01,-1.360356330871582031e+00,0.000000000000000000e+00,9.466524720191955566e-01,1.028703689575195312e+00,9.986560940742492676e-01,7.282806038856506348e-01,8.692002296447753906e-01,1.026736497879028320e+00,9.579039812088012695e-01\n",
      "1.000000000000000000e+00,1.105008959770202637e+00,3.213555514812469482e-01,1.522401213645935059e+00,8.828076124191284180e-01,-1.205349326133728027e+00,6.814661026000976562e-01,-1.070463895797729492e+00,-9.218706488609313965e-01,0.000000000000000000e+00,8.008721470832824707e-01,1.020974040031433105e+00,9.714065194129943848e-01,2.214872121810913086e+00,5.967612862586975098e-01,-3.502728641033172607e-01,6.311942934989929199e-01,0.000000000000000000e+00,4.799988865852355957e-01,-3.735655248165130615e-01,1.130406111478805542e-01,0.000000000000000000e+00,7.558564543724060059e-01,1.361057043075561523e+00,9.866096973419189453e-01,8.380846381187438965e-01,1.133295178413391113e+00,8.722448945045471191e-01,8.084865212440490723e-01\n",
      "0.000000000000000000e+00,1.595839262008666992e+00,-6.078106760978698730e-01,7.074915803968906403e-03,1.818449616432189941e+00,-1.119059920310974121e-01,8.475499153137207031e-01,-5.664370059967041016e-01,1.581239342689514160e+00,2.173076152801513672e+00,7.554209828376770020e-01,6.431096196174621582e-01,1.426366806030273438e+00,0.000000000000000000e+00,9.216607809066772461e-01,-1.190432429313659668e+00,-1.615589022636413574e+00,0.000000000000000000e+00,6.511141061782836914e-01,-6.542269587516784668e-01,-1.274344921112060547e+00,3.101961374282836914e+00,8.237605690956115723e-01,9.381914138793945312e-01,9.717581868171691895e-01,7.891763448715209961e-01,4.305532872676849365e-01,9.613569378852844238e-01,9.578179121017456055e-01\n",
      "1.000000000000000000e+00,4.093913435935974121e-01,-1.884683609008789062e+00,-1.027292013168334961e+00,1.672451734542846680e+00,-1.604598283767700195e+00,1.338014960289001465e+00,5.542744323611259460e-02,1.346588134765625000e-02,2.173076152801513672e+00,5.097832679748535156e-01,-1.038338065147399902e+00,7.078623175621032715e-01,0.000000000000000000e+00,7.469175457954406738e-01,-3.584651052951812744e-01,-1.646654248237609863e+00,0.000000000000000000e+00,3.670579791069030762e-01,6.949646025896072388e-02,1.377130270004272461e+00,3.101961374282836914e+00,8.694183826446533203e-01,1.222082972526550293e+00,1.000627398490905762e+00,5.450449585914611816e-01,6.986525058746337891e-01,9.773144721984863281e-01,8.287860751152038574e-01\n",
      "1.000000000000000000e+00,9.338953495025634766e-01,6.291297078132629395e-01,5.275348424911499023e-01,2.380327433347702026e-01,-9.665691256523132324e-01,5.478111505508422852e-01,-5.943922698497772217e-02,-1.706866145133972168e+00,2.173076152801513672e+00,9.410027265548706055e-01,-2.653732776641845703e+00,-1.572199910879135132e-01,0.000000000000000000e+00,1.030370354652404785e+00,-1.755051016807556152e-01,5.230209231376647949e-01,2.548224449157714844e+00,1.373546600341796875e+00,1.291248083114624023e+00,-1.467454433441162109e+00,0.000000000000000000e+00,9.018372893333435059e-01,1.083671212196350098e+00,9.796960949897766113e-01,7.833003997802734375e-01,8.491951823234558105e-01,8.943563103675842285e-01,7.748793959617614746e-01\n",
      "1.000000000000000000e+00,1.405143737792968750e+00,5.366026163101196289e-01,6.895543336868286133e-01,1.179567337036132812e+00,-1.100611537694931030e-01,3.202404975891113281e+00,-1.526960015296936035e+00,-1.576033473014831543e+00,0.000000000000000000e+00,2.931536912918090820e+00,5.673424601554870605e-01,-1.300333440303802490e-01,2.214872121810913086e+00,1.787122726440429688e+00,8.994985818862915039e-01,5.851513147354125977e-01,2.548224449157714844e+00,4.018652141094207764e-01,-1.512016952037811279e-01,1.163489103317260742e+00,0.000000000000000000e+00,1.667070508003234863e+00,4.039272785186767578e+00,1.175828456878662109e+00,1.045351743698120117e+00,1.542971968650817871e+00,3.534826755523681641e+00,2.740753889083862305e+00\n"
     ]
    }
   ],
   "source": [
    "!head {_data_filepath}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ONIE_hdkPS4"
   },
   "source": [
    "## Create the InteractiveContext\n",
    "An interactive context is used to provide global context when running a TFX pipeline in a notebook without using a runner or orchestrator such as Apache Airflow or Kubeflow.  This style of development is only useful when experimenting and developing the code for a pipeline, and cannot currently be used to deploy a working pipeline to production.\n",
    "\n",
    "When creating  InteractiveContext using default parameters, TFX is configured to use a temporary directory with an ephemeral ML Metadata database instance. To use your own pipeline root or database, the optional properties `pipeline_root` and `metadata_connection_config` may be passed to InteractiveContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Rh6K5sUf9dd"
   },
   "outputs": [],
   "source": [
    "context = InteractiveContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HdQWxfsVkzdJ"
   },
   "source": [
    "# Run TFX Components Interactively\n",
    "\n",
    "---\n",
    "\n",
    "In the cells that follow you will construct TFX components and run each one interactively within the InteractiveContext to obtain `ExecutionResult` objects.  This mirrors the process of an orchestrator running components in a TFX DAG based on when the dependencies for each component are met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L9fwt9gQk3BR"
   },
   "source": [
    "### The ExampleGen Component\n",
    "In any ML development process the first step when starting code development is to ingest the training and test datasets.  The `ExampleGen` component brings data into the TFX pipeline. It reads the data from the source, splits the data into a configured number of splits, converts the data into the TFRecords format, and stores the data into the configured Artifact store.\n",
    "\n",
    "In this walkthrough, you use `CsvExampleGen` that is a specialization of `ExampleGen` for CSV files. TFX provides other types of `ExampleGen` components including ones that can source data from databases and datawarehouse like BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PyXjuMt8f-9u"
   },
   "outputs": [],
   "source": [
    "input_data = external_input(_data_root)\n",
    "example_gen = CsvExampleGen(input=input_data)\n",
    "context.run(example_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0SXc2OGnDWz5"
   },
   "source": [
    "ExampleGen's outputs include 2 artifacts: the training examples and the eval examples (by default, split 2/3 training, 1/3 eval):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XoFaWckTy8pL"
   },
   "outputs": [],
   "source": [
    "for artifact in example_gen.outputs['examples'].get():\n",
    "  print(artifact.split, artifact.uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pfzF2WObDqs6"
   },
   "source": [
    "Take a peek at the output training examples to see what they look like.\n",
    "\n",
    "1. Get the URI of the output artifact representing the training examples, which is a directory\n",
    "1. Get the list of files in this directory (all compressed TFRecord files), and create a `TFRecordDataset` to read these files\n",
    "1. Iterate over the first 3 records and decode them using a `TFExampleDecoder` to check the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EEHi7dFLzZWP"
   },
   "outputs": [],
   "source": [
    "train_uri = example_gen.outputs['examples'].get()[0].uri\n",
    "tfrecord_filenames = [os.path.join(train_uri, name)\n",
    "                      for name in os.listdir(train_uri)]\n",
    "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
    "decoder = tfdv.TFExampleDecoder()\n",
    "for tfrecord in dataset.take(3):\n",
    "  serialized_example = tfrecord.numpy()\n",
    "  example = decoder.decode(serialized_example)\n",
    "  pp.pprint(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "csM6BFhtk5Aa"
   },
   "source": [
    "### The StatisticsGen Component\n",
    "The `StatisticsGen` component computes descriptive statistics for your dataset.  The statistics that it generates can be visualized for review, and are used for example validation and to infer a schema.\n",
    "\n",
    "Create a StatisticsGen component and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MAscCCYWgA-9"
   },
   "outputs": [],
   "source": [
    "statistics_gen = StatisticsGen(\n",
    "    input_data=example_gen.outputs['examples'])\n",
    "context.run(statistics_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j5LBVkeDEvZQ"
   },
   "source": [
    "Again, let's take a peek at the output training artifact. Note that this time it is a TFRecord file containing a single record with a serialized `DatasetFeatureStatisticsList` protobuf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fHP1HKDc3EXY"
   },
   "outputs": [],
   "source": [
    "train_uri = statistics_gen.outputs['statistics'].get()[0].uri\n",
    "tfrecord_filenames = [os.path.join(train_uri, name)\n",
    "                      for name in os.listdir(train_uri)]\n",
    "dataset = tf.data.TFRecordDataset(tfrecord_filenames)\n",
    "for tfrecord in dataset.take(1):\n",
    "  serialized_example = tfrecord.numpy()\n",
    "  stats = statistics_pb2.DatasetFeatureStatisticsList()\n",
    "  stats.ParseFromString(serialized_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GRNfT5_aFGJC"
   },
   "source": [
    "The statistics can be visualized using the `tfdv.visualize_statistics()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3i_3ntm3FEcK"
   },
   "outputs": [],
   "source": [
    "tfdv.visualize_statistics(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HLKLTO9Nk60p"
   },
   "source": [
    "### The SchemaGen Component\n",
    "The `SchemaGen` component generates a schema for your data based on the statistics from StatisticsGen.  It tries to infer the data types of each of your features, and the ranges of legal values for categorical features.\n",
    "\n",
    "Create a SchemaGen component and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ygQvZ6hsiQ_J"
   },
   "outputs": [],
   "source": [
    "# Generates schema based on statistics files.\n",
    "infer_schema = SchemaGen(statistics=statistics_gen.outputs['statistics'])\n",
    "context.run(infer_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kdtU3u01FR-2"
   },
   "source": [
    "The generated artifact is just a `schema.pbtxt` containing a text representation of a `schema_pb2.Schema` protobuf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L6-tgKi6A_gK"
   },
   "outputs": [],
   "source": [
    "train_uri = infer_schema.outputs['schema'].get()[0].uri\n",
    "schema_filename = os.path.join(train_uri, \"schema.pbtxt\")\n",
    "schema = tfx.utils.io_utils.parse_pbtxt_file(file_name=schema_filename,\n",
    "                                             message=schema_pb2.Schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FaSgx5qIFelw"
   },
   "source": [
    "It can be visualized using `tfdv.display_schema()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gycOsJIQFhi3"
   },
   "outputs": [],
   "source": [
    "tfdv.display_schema(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V1qcUuO9k9f8"
   },
   "source": [
    "### The ExampleValidator Component\n",
    "The `ExampleValidator` performs anomaly detection, based on the statistics from StatisticsGen and the schema from SchemaGen.  It looks for problems such as missing values, values of the wrong type, or categorical values outside of the domain of acceptable values.\n",
    "\n",
    "Create an ExampleValidator component and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XRlRUuGgiXks"
   },
   "outputs": [],
   "source": [
    "# Performs anomaly detection based on statistics and data schema.\n",
    "validate_stats = ExampleValidator(\n",
    "    statistics=statistics_gen.outputs['statistics'],\n",
    "    schema=infer_schema.outputs['schema'])\n",
    "context.run(validate_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gWUDXqADFp5U"
   },
   "source": [
    "The output artifact of `ExampleValidator` is an `anomalies.pbtxt` file describing an `anomalies_pb2.Anomalies` protobuf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DMX0LCyHCKGH"
   },
   "outputs": [],
   "source": [
    "train_uri = validate_stats.outputs['anomalies'].get()[0].uri\n",
    "anomalies_filename = os.path.join(train_uri, \"anomalies.pbtxt\")\n",
    "anomalies = tfx.utils.io_utils.parse_pbtxt_file(\n",
    "    file_name=anomalies_filename,\n",
    "    message=anomalies_pb2.Anomalies())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z-nMbzFbF22_"
   },
   "source": [
    "This can be visualized using the `tfdv.display_anomalies()` function.  Did it find any anomalies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IGfl7-8YF2Ej"
   },
   "outputs": [],
   "source": [
    "tfdv.display_anomalies(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JPViEz5RlA36"
   },
   "source": [
    "### The Transform Component\n",
    "The `Transform` component performs data transformations and feature engineering.  The results include an input TensorFlow graph which is used during both training and serving to preprocess the data before training or inference.  This graph becomes part of the SavedModel that is the result of model training.  Since the same input graph is used for both training and serving, the preprocessing will always be the same, and only needs to be written once.\n",
    "\n",
    "The Transform component requires more code than many other components because of the arbitrary complexity of the feature engineering that you may need for the data and/or model that you're working with.  It requires code files to be available which define the processing needed.\n",
    "\n",
    "Define some constants and functions for both the `Transform` component and the `Trainer` component.  Define them in a Python module, in this case saved to disk using the `%%writefile` magic command since you are working in a notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FvZ6OFHDG2fe"
   },
   "outputs": [],
   "source": [
    "_constants_module_file = 'chicago_taxi_constants.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_GpU9-JNXw-_"
   },
   "outputs": [],
   "source": [
    "%%writefile {_constants_module_file}\n",
    "\n",
    "# Categorical features are assumed to each have a maximum value in the dataset.\n",
    "MAX_CATEGORICAL_FEATURE_VALUES = [24, 31, 12]\n",
    "\n",
    "CATEGORICAL_FEATURE_KEYS = [\n",
    "    'trip_start_hour', 'trip_start_day', 'trip_start_month',\n",
    "    'pickup_census_tract', 'dropoff_census_tract', 'pickup_community_area',\n",
    "    'dropoff_community_area'\n",
    "]\n",
    "\n",
    "DENSE_FLOAT_FEATURE_KEYS = ['trip_miles', 'fare', 'trip_seconds']\n",
    "\n",
    "# Number of buckets used by tf.transform for encoding each feature.\n",
    "FEATURE_BUCKET_COUNT = 10\n",
    "\n",
    "BUCKET_FEATURE_KEYS = [\n",
    "    'pickup_latitude', 'pickup_longitude', 'dropoff_latitude',\n",
    "    'dropoff_longitude'\n",
    "]\n",
    "\n",
    "# Number of vocabulary terms used for encoding VOCAB_FEATURES by tf.transform\n",
    "VOCAB_SIZE = 1000\n",
    "\n",
    "# Count of out-of-vocab buckets in which unrecognized VOCAB_FEATURES are hashed.\n",
    "OOV_SIZE = 10\n",
    "\n",
    "VOCAB_FEATURE_KEYS = [\n",
    "    'payment_type',\n",
    "    'company',\n",
    "]\n",
    "\n",
    "# Keys\n",
    "LABEL_KEY = 'tips'\n",
    "FARE_KEY = 'fare'\n",
    "\n",
    "def transformed_name(key):\n",
    "  return key + '_xf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XUYeCayFG7kH"
   },
   "source": [
    "Now define a module containing the `preprocessing_fn()` function that will be passed to the `Transform` component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7uuWiQbOG9ki"
   },
   "outputs": [],
   "source": [
    "_transform_module_file = 'chicago_taxi_transform.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v3EIuVQnBfH7"
   },
   "outputs": [],
   "source": [
    "%%writefile {_transform_module_file}\n",
    "\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow as tf\n",
    "from chicago_taxi_constants import *\n",
    "\n",
    "\n",
    "def _transformed_names(keys):\n",
    "  return [transformed_name(key) for key in keys]\n",
    "\n",
    "\n",
    "# Tf.Transform considers these features as \"raw\"\n",
    "def _get_raw_feature_spec(schema):\n",
    "  return schema_utils.schema_as_feature_spec(schema).feature_spec\n",
    "\n",
    "\n",
    "def _gzip_reader_fn(filenames):\n",
    "  \"\"\"Small utility returning a record reader that can read gzip'ed files.\"\"\"\n",
    "  return tf.data.TFRecordDataset(\n",
    "      filenames,\n",
    "      compression_type='GZIP')\n",
    "\n",
    "\n",
    "def _fill_in_missing(x):\n",
    "  \"\"\"Replace missing values in a SparseTensor.\n",
    "  Fills in missing values of `x` with '' or 0, and converts to a dense tensor.\n",
    "  Args:\n",
    "    x: A `SparseTensor` of rank 2.  Its dense shape should have size at most 1\n",
    "      in the second dimension.\n",
    "  Returns:\n",
    "    A rank 1 tensor where missing values of `x` have been filled in.\n",
    "  \"\"\"\n",
    "  default_value = '' if x.dtype == tf.string else 0\n",
    "  return tf.squeeze(\n",
    "      tf.sparse.to_dense(\n",
    "          tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\n",
    "          default_value),\n",
    "      axis=1)\n",
    "\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "  \"\"\"tf.transform's callback function for preprocessing inputs.\n",
    "  Args:\n",
    "    inputs: map from feature keys to raw not-yet-transformed features.\n",
    "  Returns:\n",
    "    Map from string feature key to transformed feature operations.\n",
    "  \"\"\"\n",
    "  outputs = {}\n",
    "  for key in DENSE_FLOAT_FEATURE_KEYS:\n",
    "    # Preserve this feature as a dense float, setting nan's to the mean.\n",
    "    outputs[transformed_name(key)] = tft.scale_to_z_score(\n",
    "        _fill_in_missing(inputs[key]))\n",
    "\n",
    "  for key in VOCAB_FEATURE_KEYS:\n",
    "    # Build a vocabulary for this feature.\n",
    "    outputs[transformed_name(key)] = tft.compute_and_apply_vocabulary(\n",
    "        _fill_in_missing(inputs[key]),\n",
    "        top_k=VOCAB_SIZE,\n",
    "        num_oov_buckets=OOV_SIZE)\n",
    "\n",
    "  for key in BUCKET_FEATURE_KEYS:\n",
    "    outputs[transformed_name(key)] = tft.bucketize(\n",
    "        _fill_in_missing(inputs[key]), FEATURE_BUCKET_COUNT,\n",
    "        always_return_num_quantiles=False)\n",
    "\n",
    "  for key in CATEGORICAL_FEATURE_KEYS:\n",
    "    outputs[transformed_name(key)] = _fill_in_missing(inputs[key])\n",
    "\n",
    "  # Was this passenger a big tipper?\n",
    "  taxi_fare = _fill_in_missing(inputs[FARE_KEY])\n",
    "  tips = _fill_in_missing(inputs[LABEL_KEY])\n",
    "  outputs[transformed_name(LABEL_KEY)] = tf.where(\n",
    "      tf.math.is_nan(taxi_fare),\n",
    "      tf.cast(tf.zeros_like(taxi_fare), tf.int64),\n",
    "      # Test if the tip was > 20% of the fare.\n",
    "      tf.cast(\n",
    "          tf.greater(tips, tf.multiply(taxi_fare, tf.constant(0.2))), tf.int64))\n",
    "\n",
    "  return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eeMVMafpHHX1"
   },
   "source": [
    "Create and run the `Transform` component, referring to the files that were created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jHfhth_GiZI9"
   },
   "outputs": [],
   "source": [
    "# Performs transformations and feature engineering in training and serving.\n",
    "transform = Transform(\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    schema=infer_schema.outputs['schema'],\n",
    "    module_file=_transform_module_file)\n",
    "context.run(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_jbZO1ykHOeG"
   },
   "source": [
    "The `Transform` component has 2 types of outputs:\n",
    "* `transform_graph` is the graph that can perform the preprocessing operations (this graph will be included in the serving and evaluation models).\n",
    "* `transformed_examples` represents the preprocessed training and evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j4UjersvAC7p"
   },
   "outputs": [],
   "source": [
    "transform.outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wRFMlRcdHlQy"
   },
   "source": [
    "Take a peek at the `transform_graph` artifact.  It points to a directory containing 3 subdirectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E4I-cqfQQvaW"
   },
   "outputs": [],
   "source": [
    "train_uri = transform.outputs['transform_graph'].get()[0].uri\n",
    "os.listdir(train_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9374B4RpHzor"
   },
   "source": [
    "The `transform_fn` subdirectory contains the actual preprocessing graph. The `metadata` subdirectory contains the schema of the original data. The `transformed_metadata` subdirectory contains the schema of the preprocessed data.\n",
    "\n",
    "Take a look at some of the transformed examples and check that they are indeed processed as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2zIepQhSQoPa"
   },
   "outputs": [],
   "source": [
    "train_uri = transform.outputs['transformed_examples'].get()[1].uri\n",
    "tfrecord_filenames = [os.path.join(train_uri, name)\n",
    "                      for name in os.listdir(train_uri)]\n",
    "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
    "decoder = tfdv.TFExampleDecoder()\n",
    "for tfrecord in dataset.take(3):\n",
    "  serialized_example = tfrecord.numpy()\n",
    "  example = decoder.decode(serialized_example)\n",
    "  pp.pprint(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OBJFtnl6lCg9"
   },
   "source": [
    "### The Trainer Component\n",
    "The `Trainer` component trains models using TensorFlow.\n",
    "\n",
    "Create a Python module containing a `trainer_fn` function, which must return an estimator.  If you prefer creating a Keras model, you can do so and then convert it to an estimator using `keras.model_to_estimator()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d6QNYWc6PD_h"
   },
   "outputs": [],
   "source": [
    "# Setup paths.\n",
    "_trainer_module_file = 'chicago_taxi_trainer.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CaFFTBBeB4wf"
   },
   "outputs": [],
   "source": [
    "%%writefile {_trainer_module_file}\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_analysis as tfma\n",
    "import tensorflow_transform as tft\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "from chicago_taxi_constants import *\n",
    "\n",
    "\n",
    "def transformed_names(keys):\n",
    "  return [transformed_name(key) for key in keys]\n",
    "\n",
    "\n",
    "# Tf.Transform considers these features as \"raw\"\n",
    "def _get_raw_feature_spec(schema):\n",
    "  return schema_utils.schema_as_feature_spec(schema).feature_spec\n",
    "\n",
    "\n",
    "def _gzip_reader_fn(filenames):\n",
    "  \"\"\"Small utility returning a record reader that can read gzip'ed files.\"\"\"\n",
    "  return tf.data.TFRecordDataset(\n",
    "      filenames,\n",
    "      compression_type='GZIP')\n",
    "\n",
    "\n",
    "def _build_estimator(config, hidden_units=None, warm_start_from=None):\n",
    "  \"\"\"Build an estimator for predicting taxi tips\n",
    "\n",
    "  Args:\n",
    "    config: tf.estimator.RunConfig defining the runtime environment for the\n",
    "      estimator (including model_dir).\n",
    "    hidden_units: [int], the layer sizes of the DNN (input layer first)\n",
    "    warm_start_from: Optional directory to warm start from.\n",
    "\n",
    "  Returns:\n",
    "    The estimator that will be used for training and eval.\n",
    "  \"\"\"\n",
    "  real_valued_columns = [\n",
    "      tf.feature_column.numeric_column(key, shape=())\n",
    "      for key in transformed_names(DENSE_FLOAT_FEATURE_KEYS)\n",
    "  ]\n",
    "  categorical_columns = [\n",
    "      tf.feature_column.categorical_column_with_identity(\n",
    "          key, num_buckets=VOCAB_SIZE + OOV_SIZE, default_value=0)\n",
    "      for key in transformed_names(VOCAB_FEATURE_KEYS)\n",
    "  ]\n",
    "  categorical_columns += [\n",
    "      tf.feature_column.categorical_column_with_identity(\n",
    "          key, num_buckets=FEATURE_BUCKET_COUNT, default_value=0)\n",
    "      for key in transformed_names(BUCKET_FEATURE_KEYS)\n",
    "  ]\n",
    "  categorical_columns += [\n",
    "      tf.feature_column.categorical_column_with_identity(\n",
    "          key,\n",
    "          num_buckets=num_buckets,\n",
    "          default_value=0) for key, num_buckets in zip(\n",
    "              transformed_names(CATEGORICAL_FEATURE_KEYS),\n",
    "              MAX_CATEGORICAL_FEATURE_VALUES)\n",
    "  ]\n",
    "  return tf.estimator.DNNLinearCombinedRegressor(\n",
    "      config=config,\n",
    "      linear_feature_columns=categorical_columns,\n",
    "      dnn_feature_columns=real_valued_columns,\n",
    "      dnn_hidden_units=hidden_units or [100, 70, 50, 25],\n",
    "      warm_start_from=warm_start_from)\n",
    "\n",
    "\n",
    "def _example_serving_receiver_fn(tf_transform_graph, schema):\n",
    "  \"\"\"Build the serving in inputs.\n",
    "\n",
    "  Args:\n",
    "    tf_transform_graph: A TFTransformOutput.\n",
    "    schema: the schema of the input data.\n",
    "\n",
    "  Returns:\n",
    "    Tensorflow graph which parses examples, applying tf-transform to them.\n",
    "  \"\"\"\n",
    "  raw_feature_spec = _get_raw_feature_spec(schema)\n",
    "  raw_feature_spec.pop(LABEL_KEY)\n",
    "\n",
    "  raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
    "      raw_feature_spec, default_batch_size=None)\n",
    "  serving_input_receiver = raw_input_fn()\n",
    "\n",
    "  transformed_features = tf_transform_graph.transform_raw_features(\n",
    "      serving_input_receiver.features)\n",
    "\n",
    "  return tf.estimator.export.ServingInputReceiver(\n",
    "      transformed_features, serving_input_receiver.receiver_tensors)\n",
    "\n",
    "\n",
    "def _eval_input_receiver_fn(tf_transform_graph, schema):\n",
    "  \"\"\"Build everything needed for the tf-model-analysis to run the model.\n",
    "\n",
    "  Args:\n",
    "    tf_transform_graph: A TFTransformOutput.\n",
    "    schema: the schema of the input data.\n",
    "\n",
    "  Returns:\n",
    "    EvalInputReceiver function, which contains:\n",
    "      - Tensorflow graph which parses raw untransformed features, applies the\n",
    "        tf-transform preprocessing operators.\n",
    "      - Set of raw, untransformed features.\n",
    "      - Label against which predictions will be compared.\n",
    "  \"\"\"\n",
    "  # Notice that the inputs are raw features, not transformed features here.\n",
    "  raw_feature_spec = _get_raw_feature_spec(schema)\n",
    "\n",
    "  serialized_tf_example = tf.compat.v1.placeholder(\n",
    "      dtype=tf.string, shape=[None], name='input_example_tensor')\n",
    "\n",
    "  # Add a parse_example operator to the tensorflow graph, which will parse\n",
    "  # raw, untransformed, tf examples.\n",
    "  features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)\n",
    "\n",
    "  # Now that we have our raw examples, process them through the tf-transform\n",
    "  # function computed during the preprocessing step.\n",
    "  transformed_features = tf_transform_graph.transform_raw_features(\n",
    "      features)\n",
    "\n",
    "  # The key name MUST be 'examples'.\n",
    "  receiver_tensors = {'examples': serialized_tf_example}\n",
    "\n",
    "  # NOTE: Model is driven by transformed features (since training works on the\n",
    "  # materialized output of TFT, but slicing will happen on raw features.\n",
    "  features.update(transformed_features)\n",
    "\n",
    "  return tfma.export.EvalInputReceiver(\n",
    "      features=features,\n",
    "      receiver_tensors=receiver_tensors,\n",
    "      labels=transformed_features[transformed_name(LABEL_KEY)])\n",
    "\n",
    "\n",
    "def _input_fn(filenames, tf_transform_graph, batch_size=200):\n",
    "  \"\"\"Generates features and labels for training or evaluation.\n",
    "\n",
    "  Args:\n",
    "    filenames: [str] list of CSV files to read data from.\n",
    "    tf_transform_graph: A TFTransformOutput.\n",
    "    batch_size: int First dimension size of the Tensors returned by input_fn\n",
    "\n",
    "  Returns:\n",
    "    A (features, indices) tuple where features is a dictionary of\n",
    "      Tensors, and indices is a single Tensor of label indices.\n",
    "  \"\"\"\n",
    "  transformed_feature_spec = (\n",
    "      tf_transform_graph.transformed_feature_spec().copy())\n",
    "\n",
    "  dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "      filenames, batch_size, transformed_feature_spec, reader=_gzip_reader_fn)\n",
    "\n",
    "  transformed_features = dataset.make_one_shot_iterator().get_next()\n",
    "  # We pop the label because we do not want to use it as a feature while we're\n",
    "  # training.\n",
    "  return transformed_features, transformed_features.pop(\n",
    "      transformed_name(LABEL_KEY))\n",
    "\n",
    "\n",
    "# TFX will call this function\n",
    "def trainer_fn(hparams, schema):\n",
    "  \"\"\"Build the estimator using the high level API.\n",
    "  Args:\n",
    "    hparams: Holds hyperparameters used to train the model as name/value pairs.\n",
    "    schema: Holds the schema of the training examples.\n",
    "  Returns:\n",
    "    A dict of the following:\n",
    "      - estimator: The estimator that will be used for training and eval.\n",
    "      - train_spec: Spec for training.\n",
    "      - eval_spec: Spec for eval.\n",
    "      - eval_input_receiver_fn: Input function for eval.\n",
    "  \"\"\"\n",
    "  # Number of nodes in the first layer of the DNN\n",
    "  first_dnn_layer_size = 100\n",
    "  num_dnn_layers = 4\n",
    "  dnn_decay_factor = 0.7\n",
    "\n",
    "  train_batch_size = 40\n",
    "  eval_batch_size = 40\n",
    "\n",
    "  tf_transform_graph = tft.TFTransformOutput(hparams.transform_output)\n",
    "\n",
    "  train_input_fn = lambda: _input_fn(\n",
    "      hparams.train_files,\n",
    "      tf_transform_graph,\n",
    "      batch_size=train_batch_size)\n",
    "\n",
    "  eval_input_fn = lambda: _input_fn(\n",
    "      hparams.eval_files,\n",
    "      tf_transform_graph,\n",
    "      batch_size=eval_batch_size)\n",
    "\n",
    "  train_spec = tf.estimator.TrainSpec(\n",
    "      train_input_fn,\n",
    "      max_steps=hparams.train_steps)\n",
    "\n",
    "  serving_receiver_fn = lambda: _example_serving_receiver_fn(\n",
    "      tf_transform_graph, schema)\n",
    "\n",
    "  exporter = tf.estimator.FinalExporter('chicago-taxi', serving_receiver_fn)\n",
    "  eval_spec = tf.estimator.EvalSpec(\n",
    "      eval_input_fn,\n",
    "      steps=hparams.eval_steps,\n",
    "      exporters=[exporter],\n",
    "      name='chicago-taxi-eval')\n",
    "\n",
    "  run_config = tf.estimator.RunConfig(\n",
    "      save_checkpoints_steps=999, keep_checkpoint_max=1)\n",
    "\n",
    "  run_config = run_config.replace(model_dir=hparams.serving_model_dir)\n",
    "\n",
    "  estimator = _build_estimator(\n",
    "      # Construct layers sizes with exponetial decay\n",
    "      hidden_units=[\n",
    "          max(2, int(first_dnn_layer_size * dnn_decay_factor**i))\n",
    "          for i in range(num_dnn_layers)\n",
    "      ],\n",
    "      config=run_config,\n",
    "      warm_start_from=hparams.warm_start_from)\n",
    "\n",
    "  # Create an input receiver for TFMA processing\n",
    "  receiver_fn = lambda: _eval_input_receiver_fn(\n",
    "      tf_transform_graph, schema)\n",
    "\n",
    "  return {\n",
    "      'estimator': estimator,\n",
    "      'train_spec': train_spec,\n",
    "      'eval_spec': eval_spec,\n",
    "      'eval_input_receiver_fn': receiver_fn\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GnLjStUJIoos"
   },
   "source": [
    "Create and run the `Trainer` component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "429-vvCWibO0"
   },
   "outputs": [],
   "source": [
    "# Uses user-provided Python function that implements a model using TensorFlow.\n",
    "trainer = Trainer(\n",
    "    module_file=_trainer_module_file,\n",
    "    transformed_examples=transform.outputs['transformed_examples'],\n",
    "    schema=infer_schema.outputs['schema'],\n",
    "    transform_graph=transform.outputs['transform_graph'],\n",
    "    train_args=trainer_pb2.TrainArgs(num_steps=10000),\n",
    "    eval_args=trainer_pb2.EvalArgs(num_steps=5000))\n",
    "context.run(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ktJA8On9Iui7"
   },
   "source": [
    "Take a peek at the trained model which was exported from `Trainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qDBZG9Oso-BD"
   },
   "outputs": [],
   "source": [
    "train_uri = trainer.outputs['model'].get()[0].uri\n",
    "serving_model_path = os.path.join(train_uri, 'serving_model_dir', 'export', 'chicago-taxi')\n",
    "latest_serving_model_path = os.path.join(serving_model_path, max(os.listdir(serving_model_path)))\n",
    "exported_model = tf.saved_model.load(latest_serving_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KyT3ZVGCZWsj"
   },
   "outputs": [],
   "source": [
    "exported_model.graph.get_operations()[:10] + [\"...\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tgR_TkqUg-XB"
   },
   "source": [
    "## Analyze Training with TensorBoard\n",
    "Use TensorBoard to analyze the model training that was done in Trainer, and see how well our model trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wfWf5zLAUsdI"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BMZsXMklVjW0"
   },
   "outputs": [],
   "source": [
    "%tensorboard --bind_all --logdir {os.path.join(train_uri, 'serving_model_dir')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FmPftrv0lEQy"
   },
   "source": [
    "### The Evaluator Component\n",
    "The `Evaluator` component analyzes model performance using the TensorFlow Model Analysis library.  It runs inference requests on particular subsets of the test dataset, based on which `slices` are defined by the developer.  Knowing which slices should be analyzed requires domain knowledge of what is imporant in this particular use case or domain.\n",
    "\n",
    "Create and run an Evaluator component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zjcx8g6mihSt"
   },
   "outputs": [],
   "source": [
    "# Uses TFMA to compute a evaluation statistics over features of a model.\n",
    "model_analyzer = Evaluator(\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    model=trainer.outputs['model'],\n",
    "    feature_slicing_spec=evaluator_pb2.FeatureSlicingSpec(specs=[\n",
    "        evaluator_pb2.SingleSlicingSpec(\n",
    "            column_for_slicing=['weekday'])\n",
    "    ]))\n",
    "context.run(model_analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M1PVK6IhI5uS"
   },
   "outputs": [],
   "source": [
    "model_analyzer.outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kN99npvE8YdG"
   },
   "source": [
    "Use the Evaluator results to generate model performance data which can be visualized.  First create evaluation input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ioAIsxFVReR9"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "BASE_DIR = tempfile.mkdtemp()\n",
    "\n",
    "reader = csv.DictReader(open(_data_filepath))\n",
    "examples = []\n",
    "for line in reader:\n",
    "  example = tf.train.Example()\n",
    "  for feature in schema.feature:\n",
    "    key = feature.name\n",
    "    if len(line[key]) > 0:\n",
    "      if feature.type == schema_pb2.FLOAT:\n",
    "        example.features.feature[key].float_list.value[:] = [float(line[key])]\n",
    "      elif feature.type == schema_pb2.INT:\n",
    "        example.features.feature[key].int64_list.value[:] = [int(line[key])]\n",
    "      elif feature.type == schema_pb2.BYTES:\n",
    "        example.features.feature[key].bytes_list.value[:] = [line[key].encode('utf8')]\n",
    "    else:\n",
    "      if feature.type == schema_pb2.FLOAT:\n",
    "        example.features.feature[key].float_list.value[:] = []\n",
    "      elif feature.type == schema_pb2.INT:\n",
    "        example.features.feature[key].int64_list.value[:] = []\n",
    "      elif feature.type == schema_pb2.BYTES:\n",
    "        example.features.feature[key].bytes_list.value[:] = []\n",
    "  examples.append(example)\n",
    "\n",
    "TFRecord_file = os.path.join(BASE_DIR, 'train_data.rio')\n",
    "with tf.io.TFRecordWriter(TFRecord_file) as writer:\n",
    "  for example in examples:\n",
    "    writer.write(example.SerializeToString())\n",
    "  writer.flush()\n",
    "  writer.close()\n",
    "\n",
    "!ls {TFRecord_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4p6ulG6h8zUy"
   },
   "source": [
    "Run the analysis of a particular slice of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EeU2DwrcQo08"
   },
   "outputs": [],
   "source": [
    "def run_and_render(eval_model=None, slice_list=None, slice_idx=0):\n",
    "  \"\"\"Runs the model analysis and renders the slicing metrics\n",
    "\n",
    "  Args:\n",
    "      eval_model: An instance of tf.saved_model saved with evaluation data\n",
    "      slice_list: A list of tfma.slicer.SingleSliceSpec giving the slices\n",
    "      slice_idx: An integer index into slice_list specifying the slice to use\n",
    "\n",
    "  Returns:\n",
    "      A SlicingMetricsViewer object if in Jupyter notebook; None if in Colab.\n",
    "  \"\"\"\n",
    "  eval_result = tfma.run_model_analysis(eval_shared_model=eval_model,\n",
    "                                        data_location=TFRecord_file,\n",
    "                                        file_format='tfrecords',\n",
    "                                        slice_spec=slice_list,\n",
    "                                        output_path='sample_data',\n",
    "                                        extractors=None)\n",
    "  return tfma.view.render_slicing_metrics(eval_result, slicing_spec=slice_list[slice_idx] if slice_list else None)\n",
    "\n",
    "# Load the TFMA results for the first training run\n",
    "# This will take a minute\n",
    "eval_model_base_dir_0 = os.path.join(train_uri, 'eval_model_dir')\n",
    "eval_model_dir_0 = os.path.join(eval_model_base_dir_0,\n",
    "                                max(os.listdir(eval_model_base_dir_0)))\n",
    "eval_shared_model_0 = tfma.default_eval_shared_model(\n",
    "    eval_saved_model_path=eval_model_dir_0)\n",
    "\n",
    "# Slice our data by the trip_start_hour feature\n",
    "slices = [tfma.slicer.SingleSliceSpec(columns=['trip_start_hour'])]\n",
    "\n",
    "run_and_render(eval_model=eval_shared_model_0, slice_list=slices, slice_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "775VXTns9AGC"
   },
   "source": [
    "Print the slicing metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "979uI-xB9LPq"
   },
   "source": [
    "Examine the output data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "76Mil-7FlF_y"
   },
   "source": [
    "### The ModelValidator Component\n",
    "\n",
    "The `ModelValidator` component performs validation of your candidate model compared to the previously deployed model (if any) using criteria that you define, or to a baseline value.  If the new model scores better than the previous model it will be \"blessed\" by ModelValidator, approving it for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FXk1MA7sijCr"
   },
   "outputs": [],
   "source": [
    "# Performs quality validation of a candidate model (compared to a baseline).\n",
    "model_validator = ModelValidator(\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    model=trainer.outputs['model'])\n",
    "context.run(model_validator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FKLmZtJ695Hg"
   },
   "source": [
    "Examine the output of ModelValidator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U-si25tpAQ0q"
   },
   "outputs": [],
   "source": [
    "model_validator.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z2RWZWD-AZ-u"
   },
   "outputs": [],
   "source": [
    "blessing_uri = model_validator.outputs['blessing'].get()[0].uri\n",
    "!ls -l {blessing_uri}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T8DYekCZlHfj"
   },
   "source": [
    "### The Pusher Component\n",
    "The `Pusher` component checks whether a model has been \"blessed\", and if so, deploys it to production by pushing the model to a well known file destination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KvVasBxePW-n"
   },
   "outputs": [],
   "source": [
    "# Setup serving path\n",
    "_serving_model_dir = os.path.join(tempfile.mkdtemp(),\n",
    "                                  'serving_model/chicago_taxi_simple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pXxIa4r4-pim"
   },
   "source": [
    "Create and run a Pusher component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r45nQ69eikc9"
   },
   "outputs": [],
   "source": [
    "# Checks whether the model passed the validation steps and pushes the model\n",
    "# to a file destination if check passed.\n",
    "pusher = Pusher(\n",
    "    model=trainer.outputs['model'],\n",
    "    model_blessing=model_validator.outputs['blessing'],\n",
    "    push_destination=pusher_pb2.PushDestination(\n",
    "        filesystem=pusher_pb2.PushDestination.Filesystem(\n",
    "            base_directory=_serving_model_dir)))\n",
    "context.run(pusher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VmPKLvoB-vy0"
   },
   "source": [
    "Examine the output of Pusher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gNvMj9AWsmSt"
   },
   "outputs": [],
   "source": [
    "pusher.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4bphCjS-B4vv"
   },
   "outputs": [],
   "source": [
    "push_uri = pusher.outputs['pushed_model'].get()[0].uri\n",
    "latest_version = max(os.listdir(push_uri))\n",
    "latest_version_path = os.path.join(push_uri, latest_version)\n",
    "model = tf.saved_model.load(latest_version_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hQAmjZ81B8xm"
   },
   "outputs": [],
   "source": [
    "for item in model.signatures.items():\n",
    "  pp.pprint(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e5cKQjPJX0IW"
   },
   "source": [
    "##TensorFlow Serving\n",
    "\n",
    "Now that we have a trained model that has been blessed by ModelValidator, and pushed to our deployment target by Pusher, we can load it into TensorFlow Serving and start serving inference requests.\n",
    "\n",
    "#### Examine your saved model\n",
    "\n",
    "We'll use the command line utility `saved_model_cli` to look at the [MetaGraphDefs](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/MetaGraphDef) (the models) and [SignatureDefs](../signature_defs) (the methods you can call) in our SavedModel.  See [this discussion of the SavedModel CLI](https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/saved_model.md#cli-to-inspect-and-execute-savedmodel) in the TensorFlow Guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h3YIGoRuX0-J"
   },
   "outputs": [],
   "source": [
    "latest_pushed_model = os.path.join(_serving_model_dir, max(os.listdir(_serving_model_dir)))\n",
    "!saved_model_cli show --dir {latest_pushed_model} --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BxikHooyYBjM"
   },
   "source": [
    "That tells us a lot about our model!  In this case we just trained our model, so we already know the inputs and outputs, but if we didn't this would be important information.  It doesn't tell us everything, but it's a great start.\n",
    "\n",
    "### Add TensorFlow Serving distribution URI as a package source:\n",
    "\n",
    "We're preparing to install TensorFlow Serving using [Aptitude](https://wiki.debian.org/Aptitude) since this Colab runs in a Debian environment.  We'll add the `tensorflow-model-server` package to the list of packages that Aptitude knows about.  Note that we're running as root.\n",
    "\n",
    "Note: This example is running TensorFlow Serving natively, but [you can also run it in a Docker container](https://www.tensorflow.org/tfx/serving/docker), which is one of the easiest ways to get started using TensorFlow Serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uG_juAPUYCbI"
   },
   "outputs": [],
   "source": [
    "# This is the same as you would do from your command line, but without the [arch=amd64], and no sudo\n",
    "# You would instead do:\n",
    "# echo \"deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | sudo tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\n",
    "# curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | sudo apt-key add -\n",
    "\n",
    "!echo \"deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\n",
    "curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | apt-key add -\n",
    "!apt update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uAw4M8jIYRLz"
   },
   "source": [
    "### Install TensorFlow Serving\n",
    "\n",
    "This is all you need - one command line!  Please note that running TensorFlow Serving in a Docker Container is also a great option, with a lot of advantages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i5D9eXsAYVMQ"
   },
   "outputs": [],
   "source": [
    "!apt-get install tensorflow-model-server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wBtbWxkEYdaj"
   },
   "source": [
    "### Start running TensorFlow Serving\n",
    "\n",
    "This is where we start running TensorFlow Serving and load our model.  After it loads we can start making inference requests using REST.  There are some important parameters:\n",
    "\n",
    "* `rest_api_port`: The port that you'll use for REST requests.\n",
    "* `model_name`: You'll use this in the URL of REST requests.  It can be anything.\n",
    "* `model_base_path`: This is the path to the directory where you've saved your model. Note that this base_path should _not_ include the model version directory, which is why we split it off below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x_Pe6MtMYh1N"
   },
   "outputs": [],
   "source": [
    "os.environ[\"MODEL_DIR\"] = os.path.split(latest_pushed_model)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k31wLFSVYl-R"
   },
   "outputs": [],
   "source": [
    "%%bash --bg \n",
    "nohup tensorflow_model_server \\\n",
    "  --rest_api_port=8501 \\\n",
    "  --model_name=chicago_taxi_simple \\\n",
    "  --model_base_path=\"${MODEL_DIR}\" >server.log 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O42M65ArYo2K"
   },
   "outputs": [],
   "source": [
    "!tail server.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vgaKdyI_Yugw"
   },
   "source": [
    "### Prepare data for inference requests\n",
    "\n",
    "Our example data is stored in a CSV file on disk.\n",
    "We first have to read the file and decode the examples from CSV, and then encode these as Example protos to feed to Tensorflow Serving.\n",
    "\n",
    "A few notes:\n",
    "\n",
    "* The `regress` and `classify` APIs are higher-level and thus encouraged to be used over `predict` - here we use the `predict` API to showcase the more involved route.\n",
    "* While the `regress` and `classify` APIs expect and can parse `tf.Example`, the `predict` API expects arbitrary `TensorProto`. This means we will have to construct the `tf.Example` proto using coders from Tensorflow Transform.\n",
    "* The REST API surface accepts JSON, which uses UTF-8 encoding. Thus to access the model via REST, we will encode our serialized `tf.Example` using Base64.\n",
    "* This is quite complicated and in general, if using the `predict` API, you should strongly consider using the gRPC API surface. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZD6IKBFwYyQ1"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "import requests\n",
    "\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.tf_metadata import dataset_schema\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "from tensorflow_transform import coders as tft_coders\n",
    "\n",
    "from chicago_taxi_constants import * \n",
    "\n",
    "def _get_raw_feature_spec(schema):\n",
    "  return schema_utils.schema_as_feature_spec(schema).feature_spec\n",
    "\n",
    "def _make_proto_coder(schema):\n",
    "  raw_feature_spec = _get_raw_feature_spec(schema)\n",
    "  raw_schema = dataset_schema.from_feature_spec(raw_feature_spec)\n",
    "  return tft_coders.ExampleProtoCoder(raw_schema)\n",
    "\n",
    "def _make_csv_coder(schema, column_names):\n",
    "  \"\"\"Return a coder for tf.transform to read csv files.\"\"\"\n",
    "  raw_feature_spec = _get_raw_feature_spec(schema)\n",
    "  parsing_schema = dataset_schema.from_feature_spec(raw_feature_spec)\n",
    "  return tft_coders.CsvCoder(column_names, parsing_schema)\n",
    "\n",
    "def make_serialized_examples(examples_csv_file, num_examples, schema):\n",
    "  \"\"\"Parses examples from CSV file and returns seralized proto examples.\"\"\"\n",
    "  filtered_features = [\n",
    "      feature for feature in schema.feature if feature.name != LABEL_KEY\n",
    "  ]\n",
    "  del schema.feature[:]\n",
    "  schema.feature.extend(filtered_features)\n",
    "\n",
    "  columns = tfx.utils.io_utils.load_csv_column_names(examples_csv_file)\n",
    "  csv_coder = _make_csv_coder(schema, columns)\n",
    "  proto_coder = _make_proto_coder(schema)\n",
    "\n",
    "  input_file = open(examples_csv_file, 'r')\n",
    "  input_file.readline()  # skip header line\n",
    "\n",
    "  serialized_examples = []\n",
    "  for _ in range(num_examples):\n",
    "    one_line = input_file.readline()\n",
    "    if not one_line:\n",
    "      print('End of example file reached')\n",
    "      break\n",
    "    one_example = csv_coder.decode(one_line)\n",
    "\n",
    "    serialized_example = proto_coder.encode(one_example)\n",
    "    serialized_examples.append(serialized_example)\n",
    "  \n",
    "  return serialized_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LPBt40HnY9rI"
   },
   "source": [
    "### Perform Inference on example data\n",
    "\n",
    "Prepare the example data using the utility defined above and batch all requests together to send a single REST API call to Tensorflow Serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Xs5x8S1Y-Yd"
   },
   "outputs": [],
   "source": [
    "def do_inference(server_addr, model_name, serialized_examples):\n",
    "  \"\"\"Sends requests to the model and prints the results.\n",
    "  Args:\n",
    "    server_addr: network address of model server in \"host:port\" format\n",
    "    model_name: name of the model as understood by the model server\n",
    "    serialized_examples: serialized examples of data to do inference on\n",
    "  \"\"\"\n",
    "  parsed_server_addr = server_addr.split(':')\n",
    "\n",
    "  host=parsed_server_addr[0]\n",
    "  port=parsed_server_addr[1]\n",
    "  json_examples = []\n",
    "  \n",
    "  for serialized_example in serialized_examples:\n",
    "    # The encoding follows the guidelines in:\n",
    "    # https://www.tensorflow.org/tfx/serving/api_rest\n",
    "    example_bytes = base64.b64encode(serialized_example).decode('utf-8')\n",
    "    predict_request = '{ \"b64\": \"%s\" }' % example_bytes\n",
    "    json_examples.append(predict_request)\n",
    "\n",
    "  json_request = '{ \"instances\": [' + ','.join(map(str, json_examples)) + ']}'\n",
    "\n",
    "  server_url = 'http://' + host + ':' + port + '/v1/models/' + model_name + ':predict'\n",
    "  response = requests.post(\n",
    "      server_url, data=json_request, timeout=5.0)\n",
    "  response.raise_for_status()\n",
    "  prediction = response.json()\n",
    "  print(json.dumps(prediction, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JOs21jlaZGWi"
   },
   "outputs": [],
   "source": [
    "serialized_examples = make_serialized_examples(\n",
    "    examples_csv_file=_data_filepath, \n",
    "    num_examples=3, \n",
    "    schema=schema)\n",
    "\n",
    "do_inference(server_addr='127.0.0.1:8501', \n",
    "     model_name='chicago_taxi_simple',\n",
    "     serialized_examples=serialized_examples)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "wdeKOEkv1Fe8"
   ],
   "name": "TFX Workshop Colab",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tfx",
   "language": "python",
   "name": "tfx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
