apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  annotations:
    pipelines.kubeflow.org/pipeline_spec: '{"description": "CLV Training Pipeline
      using BigQuery for feature engineering and Automl Tables for model training",
      "inputs": [{"name": "project_id"}, {"default": "WITH\n  order_summaries as (\n    SELECT\n      a.customer_id,\n      a.order_date,\n      a.order_value,\n      a.order_qty_articles\n    FROM\n    (\n      SELECT\n        customer_id,\n        order_date,\n        ROUND(SUM(unit_price
      * quantity), 2) AS order_value,\n        SUM(quantity) AS order_qty_articles,\n        (\n          SELECT\n            MAX(order_date)\n          FROM\n            `jk-caip.lab_201.transactions`
      tl\n          WHERE\n            tl.customer_id = t.customer_id\n        ) latest_order\n      FROM\n        `jk-caip.lab_201.transactions`
      t\n      GROUP BY\n          customer_id,\n          order_date\n    ) a\n\n    INNER
      JOIN (\n      -- Only customers with more than one positive order values before
      threshold.\n      SELECT\n        customer_id\n      FROM (\n        -- Customers
      and how many positive order values  before threshold.\n        SELECT\n          customer_id,\n          SUM(positive_value)
      cnt_positive_value\n        FROM (\n          -- Customer with whether order
      was positive or not at each date.\n          SELECT\n            customer_id,\n            (\n              CASE\n                WHEN
      SUM(unit_price * quantity) > 0 THEN 1\n                ELSE 0\n              END
      ) positive_value\n          FROM\n            `jk-caip.lab_201.transactions`\n          WHERE\n            order_date
      < DATE(\"2011-08-08\")\n          GROUP BY\n            customer_id,\n            order_date)\n        GROUP
      BY\n          customer_id )\n      WHERE\n        cnt_positive_value > 1\n      )
      b\n    ON\n      a.customer_id = b.customer_id\n    --[START common_clean]\n    WHERE\n      --
      Bought in the past 3 months\n      DATE_DIFF(DATE(\"2011-12-12\"), latest_order,
      DAY) <= 90\n      -- Make sure returns are consistent.\n      AND (\n        (order_qty_articles
      > 0 and order_Value > 0) OR\n        (order_qty_articles < 0 and order_Value
      < 0)\n      ))\n          \nSELECT\n--  tf.customer_id,\n  ROUND(tf.monetary,
      2) as monetary,\n  tf.cnt_orders AS frequency,\n  tf.recency,\n  tf.T,\n  ROUND(tf.recency/cnt_orders,
      2) AS time_between,\n  ROUND(tf.avg_basket_value, 2) AS avg_basket_value,\n  ROUND(tf.avg_basket_size,
      2) AS avg_basket_size,\n  tf.cnt_returns,\n  -- Target calculated for overall
      period\n  ROUND(tt.target_monetary, 2) as target_monetary\nFROM\n  -- This SELECT
      uses only data before threshold to make features.\n  (\n    SELECT\n      customer_id,\n      SUM(order_value)
      AS monetary,\n      DATE_DIFF(MAX(order_date), MIN(order_date), DAY) AS recency,\n      DATE_DIFF(DATE(''2011-08-08''),
      MIN(order_date), DAY) AS T,\n      COUNT(DISTINCT order_date) AS cnt_orders,\n      AVG(order_qty_articles)
      avg_basket_size,\n      AVG(order_value) avg_basket_value,\n      SUM(CASE\n          WHEN
      order_value < 1 THEN 1\n          ELSE 0 END) AS cnt_returns\n    FROM\n      order_summaries
      a\n    WHERE\n      order_date <= DATE(''2011-08-08'')\n    GROUP BY\n      customer_id)
      tf,\n\n  -- This SELECT uses data after threshold to calculate the target )\n  (\n    SELECT\n      customer_id,\n      SUM(order_value)
      target_monetary\n    FROM\n      order_summaries\n      WHERE order_date > DATE(''2011-08-08'')\n    GROUP
      BY\n      customer_id) tt\nWHERE\n  tf.customer_id = tt.customer_id\n  AND tf.monetary
      > 0\n  AND tf.monetary <= 15000", "name": "feature_engineering_query"}, {"default":
      "us-central1", "name": "aml_compute_region"}, {"default": "features", "name":
      "features_table_id"}, {"default": "lab_201", "name": "features_dataset_id"},
      {"default": "US", "name": "features_dataset_location"}, {"default": "clv_features",
      "name": "aml_dataset_name"}, {"default": "target_monetary", "name": "target_column_name"},
      {"default": "clv_regression", "name": "aml_model_name"}, {"default": "1000",
      "name": "train_budget", "type": "Integer"}, {"default": "MINIMIZE_MAE", "name":
      "optimization_objective"}, {"default": "mean_absolute_error", "name": "primary_metric"},
      {"default": "900", "name": "deployment_threshold", "type": "Float"}], "name":
      "CLV Training"}'
  generateName: clv-training-
spec:
  arguments:
    parameters:
    - name: project-id
    - name: feature-engineering-query
      value: "WITH\n  order_summaries as (\n    SELECT\n      a.customer_id,\n   \
        \   a.order_date,\n      a.order_value,\n      a.order_qty_articles\n    FROM\n\
        \    (\n      SELECT\n        customer_id,\n        order_date,\n        ROUND(SUM(unit_price\
        \ * quantity), 2) AS order_value,\n        SUM(quantity) AS order_qty_articles,\n\
        \        (\n          SELECT\n            MAX(order_date)\n          FROM\n\
        \            `jk-caip.lab_201.transactions` tl\n          WHERE\n        \
        \    tl.customer_id = t.customer_id\n        ) latest_order\n      FROM\n\
        \        `jk-caip.lab_201.transactions` t\n      GROUP BY\n          customer_id,\n\
        \          order_date\n    ) a\n\n    INNER JOIN (\n      -- Only customers\
        \ with more than one positive order values before threshold.\n      SELECT\n\
        \        customer_id\n      FROM (\n        -- Customers and how many positive\
        \ order values  before threshold.\n        SELECT\n          customer_id,\n\
        \          SUM(positive_value) cnt_positive_value\n        FROM (\n      \
        \    -- Customer with whether order was positive or not at each date.\n  \
        \        SELECT\n            customer_id,\n            (\n              CASE\n\
        \                WHEN SUM(unit_price * quantity) > 0 THEN 1\n            \
        \    ELSE 0\n              END ) positive_value\n          FROM\n        \
        \    `jk-caip.lab_201.transactions`\n          WHERE\n            order_date\
        \ < DATE(\"2011-08-08\")\n          GROUP BY\n            customer_id,\n \
        \           order_date)\n        GROUP BY\n          customer_id )\n     \
        \ WHERE\n        cnt_positive_value > 1\n      ) b\n    ON\n      a.customer_id\
        \ = b.customer_id\n    --[START common_clean]\n    WHERE\n      -- Bought\
        \ in the past 3 months\n      DATE_DIFF(DATE(\"2011-12-12\"), latest_order,\
        \ DAY) <= 90\n      -- Make sure returns are consistent.\n      AND (\n  \
        \      (order_qty_articles > 0 and order_Value > 0) OR\n        (order_qty_articles\
        \ < 0 and order_Value < 0)\n      ))\n          \nSELECT\n--  tf.customer_id,\n\
        \  ROUND(tf.monetary, 2) as monetary,\n  tf.cnt_orders AS frequency,\n  tf.recency,\n\
        \  tf.T,\n  ROUND(tf.recency/cnt_orders, 2) AS time_between,\n  ROUND(tf.avg_basket_value,\
        \ 2) AS avg_basket_value,\n  ROUND(tf.avg_basket_size, 2) AS avg_basket_size,\n\
        \  tf.cnt_returns,\n  -- Target calculated for overall period\n  ROUND(tt.target_monetary,\
        \ 2) as target_monetary\nFROM\n  -- This SELECT uses only data before threshold\
        \ to make features.\n  (\n    SELECT\n      customer_id,\n      SUM(order_value)\
        \ AS monetary,\n      DATE_DIFF(MAX(order_date), MIN(order_date), DAY) AS\
        \ recency,\n      DATE_DIFF(DATE('2011-08-08'), MIN(order_date), DAY) AS T,\n\
        \      COUNT(DISTINCT order_date) AS cnt_orders,\n      AVG(order_qty_articles)\
        \ avg_basket_size,\n      AVG(order_value) avg_basket_value,\n      SUM(CASE\n\
        \          WHEN order_value < 1 THEN 1\n          ELSE 0 END) AS cnt_returns\n\
        \    FROM\n      order_summaries a\n    WHERE\n      order_date <= DATE('2011-08-08')\n\
        \    GROUP BY\n      customer_id) tf,\n\n  -- This SELECT uses data after\
        \ threshold to calculate the target )\n  (\n    SELECT\n      customer_id,\n\
        \      SUM(order_value) target_monetary\n    FROM\n      order_summaries\n\
        \      WHERE order_date > DATE('2011-08-08')\n    GROUP BY\n      customer_id)\
        \ tt\nWHERE\n  tf.customer_id = tt.customer_id\n  AND tf.monetary > 0\n  AND\
        \ tf.monetary <= 15000"
    - name: aml-compute-region
      value: us-central1
    - name: features-table-id
      value: features
    - name: features-dataset-id
      value: lab_201
    - name: features-dataset-location
      value: US
    - name: aml-dataset-name
      value: clv_features
    - name: target-column-name
      value: target_monetary
    - name: aml-model-name
      value: clv_regression
    - name: train-budget
      value: '1000'
    - name: optimization-objective
      value: MINIMIZE_MAE
    - name: primary-metric
      value: mean_absolute_error
    - name: deployment-threshold
      value: '900'
  entrypoint: clv-training
  serviceAccountName: pipeline-runner
  templates:
  - container:
      args:
      - --gcp-project-id
      - '{{inputs.parameters.project-id}}'
      - --gcp-region
      - '{{inputs.parameters.aml-compute-region}}'
      - --display-name
      - '{{inputs.parameters.aml-dataset-name}}'
      - '----output-paths'
      - /tmp/outputs/dataset_path/data
      - /tmp/outputs/create_time/data
      - /tmp/outputs/dataset_id/data
      command:
      - python3
      - -u
      - -c
      - "from typing import NamedTuple\n\ndef automl_create_dataset_for_tables(\n\
        \    gcp_project_id: str,\n    gcp_region: str,\n    display_name: str,\n\
        \    description: str = None,\n    tables_dataset_metadata: dict = {},\n \
        \   retry=None, #=google.api_core.gapic_v1.method.DEFAULT,\n    timeout: float\
        \ = None, #=google.api_core.gapic_v1.method.DEFAULT,\n    metadata: dict =\
        \ None,\n) -> NamedTuple('Outputs', [('dataset_path', str), ('create_time',\
        \ str), ('dataset_id', str)]):\n    '''automl_create_dataset_for_tables creates\
        \ an empty Dataset for AutoML tables\n    '''\n    import sys\n    import\
        \ subprocess\n    subprocess.run([sys.executable, '-m', 'pip', 'install',\
        \ 'google-cloud-automl==0.4.0', '--quiet', '--no-warn-script-location'], env={'PIP_DISABLE_PIP_VERSION_CHECK':\
        \ '1'}, check=True)\n\n    import google\n    from google.cloud import automl\n\
        \    client = automl.AutoMlClient()\n\n    location_path = client.location_path(gcp_project_id,\
        \ gcp_region)\n    dataset_dict = {\n        'display_name': display_name,\n\
        \        'description': description,\n        'tables_dataset_metadata': tables_dataset_metadata,\n\
        \    }\n    dataset = client.create_dataset(\n        location_path,\n   \
        \     dataset_dict,\n        retry or google.api_core.gapic_v1.method.DEFAULT,\n\
        \        timeout or google.api_core.gapic_v1.method.DEFAULT,\n        metadata,\n\
        \    )\n    print(dataset)\n    dataset_id = dataset.name.rsplit('/', 1)[-1]\n\
        \    return (dataset.name, dataset.create_time, dataset_id)\n\nimport json\n\
        import argparse\n_missing_arg = object()\n_parser = argparse.ArgumentParser(prog='Automl\
        \ create dataset for tables', description='automl_create_dataset_for_tables\
        \ creates an empty Dataset for AutoML tables\\n')\n_parser.add_argument(\"\
        --gcp-project-id\", dest=\"gcp_project_id\", type=str, required=True, default=_missing_arg)\n\
        _parser.add_argument(\"--gcp-region\", dest=\"gcp_region\", type=str, required=True,\
        \ default=_missing_arg)\n_parser.add_argument(\"--display-name\", dest=\"\
        display_name\", type=str, required=True, default=_missing_arg)\n_parser.add_argument(\"\
        --description\", dest=\"description\", type=str, required=False, default=_missing_arg)\n\
        _parser.add_argument(\"--tables-dataset-metadata\", dest=\"tables_dataset_metadata\"\
        , type=json.loads, required=False, default=_missing_arg)\n_parser.add_argument(\"\
        --retry\", dest=\"retry\", type=str, required=False, default=_missing_arg)\n\
        _parser.add_argument(\"--timeout\", dest=\"timeout\", type=float, required=False,\
        \ default=_missing_arg)\n_parser.add_argument(\"--metadata\", dest=\"metadata\"\
        , type=json.loads, required=False, default=_missing_arg)\n_parser.add_argument(\"\
        ----output-paths\", dest=\"_output_paths\", type=str, nargs=3)\n_parsed_args\
        \ = {k: v for k, v in vars(_parser.parse_args()).items() if v is not _missing_arg}\n\
        _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = automl_create_dataset_for_tables(**_parsed_args)\n\
        \nif not hasattr(_outputs, '__getitem__') or isinstance(_outputs, str):\n\
        \    _outputs = [_outputs]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
        \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
        \        pass\n    with open(output_file, 'w') as f:\n        f.write(str(_outputs[idx]))\n"
      env:
      - name: GOOGLE_APPLICATION_CREDENTIALS
        value: /secret/gcp-credentials/user-gcp-sa.json
      - name: CLOUDSDK_AUTH_CREDENTIAL_FILE_OVERRIDE
        value: /secret/gcp-credentials/user-gcp-sa.json
      image: python:3.7
      volumeMounts:
      - mountPath: /secret/gcp-credentials
        name: gcp-credentials-user-gcp-sa
    inputs:
      parameters:
      - name: aml-compute-region
      - name: aml-dataset-name
      - name: project-id
    metadata:
      annotations:
        pipelines.kubeflow.org/component_spec: '{"description": "automl_create_dataset_for_tables
          creates an empty Dataset for AutoML tables\n", "inputs": [{"name": "gcp_project_id",
          "type": "String"}, {"name": "gcp_region", "type": "String"}, {"name": "display_name",
          "type": "String"}, {"name": "description", "optional": true, "type": "String"},
          {"default": "{}", "name": "tables_dataset_metadata", "optional": true, "type":
          "JsonObject"}, {"name": "retry", "optional": true}, {"name": "timeout",
          "optional": true, "type": "Float"}, {"name": "metadata", "optional": true,
          "type": "JsonObject"}], "name": "Automl create dataset for tables", "outputs":
          [{"name": "dataset_path", "type": "String"}, {"name": "create_time", "type":
          "String"}, {"name": "dataset_id", "type": "String"}]}'
    name: automl-create-dataset-for-tables
    outputs:
      artifacts:
      - name: automl-create-dataset-for-tables-create-time
        path: /tmp/outputs/create_time/data
      - name: automl-create-dataset-for-tables-dataset-id
        path: /tmp/outputs/dataset_id/data
      - name: automl-create-dataset-for-tables-dataset-path
        path: /tmp/outputs/dataset_path/data
      parameters:
      - name: automl-create-dataset-for-tables-dataset-id
        valueFrom:
          path: /tmp/outputs/dataset_id/data
      - name: automl-create-dataset-for-tables-dataset-path
        valueFrom:
          path: /tmp/outputs/dataset_path/data
    volumes:
    - name: gcp-credentials-user-gcp-sa
      secret:
        secretName: user-gcp-sa
  - container:
      args:
      - --gcp-project-id
      - '{{inputs.parameters.project-id}}'
      - --gcp-region
      - '{{inputs.parameters.aml-compute-region}}'
      - --display-name
      - '{{inputs.parameters.aml-model-name}}'
      - --dataset-id
      - '{{inputs.parameters.automl-create-dataset-for-tables-dataset-id}}'
      - --target-column-path
      - '{{inputs.parameters.automl-split-dataset-table-column-names-target-column-path}}'
      - --input-feature-column-paths
      - '{{inputs.parameters.automl-split-dataset-table-column-names-feature-column-paths}}'
      - --optimization-objective
      - '{{inputs.parameters.optimization-objective}}'
      - --train-budget-milli-node-hours
      - '{{inputs.parameters.train-budget}}'
      - '----output-paths'
      - /tmp/outputs/model_path/data
      - /tmp/outputs/model_id/data
      command:
      - python3
      - -u
      - -c
      - "from typing import NamedTuple\n\ndef automl_create_model_for_tables(\n  \
        \  gcp_project_id: str,\n    gcp_region: str,\n    display_name: str,\n  \
        \  dataset_id: str,\n    target_column_path: str = None,\n    input_feature_column_paths:\
        \ list = None,\n    optimization_objective: str = 'MAXIMIZE_AU_PRC',\n   \
        \ train_budget_milli_node_hours: int = 1000,\n) -> NamedTuple('Outputs', [('model_path',\
        \ str), ('model_id', str)]):\n    import sys\n    import subprocess\n    subprocess.run([sys.executable,\
        \ '-m', 'pip', 'install', 'google-cloud-automl==0.4.0', '--quiet', '--no-warn-script-location'],\
        \ env={'PIP_DISABLE_PIP_VERSION_CHECK': '1'}, check=True)\n\n    from google.cloud\
        \ import automl\n    client = automl.AutoMlClient()\n\n    location_path =\
        \ client.location_path(gcp_project_id, gcp_region)\n    model_dict = {\n \
        \       'display_name': display_name,\n        'dataset_id': dataset_id,\n\
        \        'tables_model_metadata': {\n            'target_column_spec': automl.types.ColumnSpec(name=target_column_path),\n\
        \            'input_feature_column_specs': [automl.types.ColumnSpec(name=path)\
        \ for path in input_feature_column_paths] if input_feature_column_paths else\
        \ None,\n            'optimization_objective': optimization_objective,\n \
        \           'train_budget_milli_node_hours': train_budget_milli_node_hours,\n\
        \        },\n    }\n\n    create_model_response = client.create_model(location_path,\
        \ model_dict)\n    print('Create model operation: {}'.format(create_model_response.operation))\n\
        \    result = create_model_response.result()\n    print(result)\n    model_name\
        \ = result.name\n    model_id = model_name.rsplit('/', 1)[-1]\n    return\
        \ (model_name, model_id)\n\nimport json\nimport argparse\n_missing_arg = object()\n\
        _parser = argparse.ArgumentParser(prog='Automl create model for tables', description='')\n\
        _parser.add_argument(\"--gcp-project-id\", dest=\"gcp_project_id\", type=str,\
        \ required=True, default=_missing_arg)\n_parser.add_argument(\"--gcp-region\"\
        , dest=\"gcp_region\", type=str, required=True, default=_missing_arg)\n_parser.add_argument(\"\
        --display-name\", dest=\"display_name\", type=str, required=True, default=_missing_arg)\n\
        _parser.add_argument(\"--dataset-id\", dest=\"dataset_id\", type=str, required=True,\
        \ default=_missing_arg)\n_parser.add_argument(\"--target-column-path\", dest=\"\
        target_column_path\", type=str, required=False, default=_missing_arg)\n_parser.add_argument(\"\
        --input-feature-column-paths\", dest=\"input_feature_column_paths\", type=json.loads,\
        \ required=False, default=_missing_arg)\n_parser.add_argument(\"--optimization-objective\"\
        , dest=\"optimization_objective\", type=str, required=False, default=_missing_arg)\n\
        _parser.add_argument(\"--train-budget-milli-node-hours\", dest=\"train_budget_milli_node_hours\"\
        , type=int, required=False, default=_missing_arg)\n_parser.add_argument(\"\
        ----output-paths\", dest=\"_output_paths\", type=str, nargs=2)\n_parsed_args\
        \ = {k: v for k, v in vars(_parser.parse_args()).items() if v is not _missing_arg}\n\
        _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = automl_create_model_for_tables(**_parsed_args)\n\
        \nif not hasattr(_outputs, '__getitem__') or isinstance(_outputs, str):\n\
        \    _outputs = [_outputs]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
        \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
        \        pass\n    with open(output_file, 'w') as f:\n        f.write(str(_outputs[idx]))\n"
      env:
      - name: GOOGLE_APPLICATION_CREDENTIALS
        value: /secret/gcp-credentials/user-gcp-sa.json
      - name: CLOUDSDK_AUTH_CREDENTIAL_FILE_OVERRIDE
        value: /secret/gcp-credentials/user-gcp-sa.json
      image: python:3.7
      volumeMounts:
      - mountPath: /secret/gcp-credentials
        name: gcp-credentials-user-gcp-sa
    inputs:
      parameters:
      - name: aml-compute-region
      - name: aml-model-name
      - name: automl-create-dataset-for-tables-dataset-id
      - name: automl-split-dataset-table-column-names-feature-column-paths
      - name: automl-split-dataset-table-column-names-target-column-path
      - name: optimization-objective
      - name: project-id
      - name: train-budget
    metadata:
      annotations:
        pipelines.kubeflow.org/component_spec: '{"inputs": [{"name": "gcp_project_id",
          "type": "String"}, {"name": "gcp_region", "type": "String"}, {"name": "display_name",
          "type": "String"}, {"name": "dataset_id", "type": "String"}, {"name": "target_column_path",
          "optional": true, "type": "String"}, {"name": "input_feature_column_paths",
          "optional": true, "type": "JsonArray"}, {"default": "MAXIMIZE_AU_PRC", "name":
          "optimization_objective", "optional": true, "type": "String"}, {"default":
          "1000", "name": "train_budget_milli_node_hours", "optional": true, "type":
          "Integer"}], "name": "Automl create model for tables", "outputs": [{"name":
          "model_path", "type": "String"}, {"name": "model_id", "type": "String"}]}'
    name: automl-create-model-for-tables
    outputs:
      artifacts:
      - name: automl-create-model-for-tables-model-id
        path: /tmp/outputs/model_id/data
      - name: automl-create-model-for-tables-model-path
        path: /tmp/outputs/model_path/data
      parameters:
      - name: automl-create-model-for-tables-model-path
        valueFrom:
          path: /tmp/outputs/model_path/data
    volumes:
    - name: gcp-credentials-user-gcp-sa
      secret:
        secretName: user-gcp-sa
  - container:
      args:
      - --model-path
      - '{{inputs.parameters.automl-create-model-for-tables-model-path}}'
      command:
      - python3
      - -u
      - -c
      - "def automl_deploy_model(model_path: str):\n\n    import logging\n    from\
        \ google.cloud import automl_v1beta1 as automl\n    from google.cloud.automl_v1beta1\
        \ import enums\n\n    logging.basicConfig(level=logging.INFO)\n    client\
        \ = automl.TablesClient()\n\n    model = client.get_model(model_name=model_path)\n\
        \    if model.deployment_state != enums.Model.DeploymentState.DEPLOYED:\n\
        \        logging.info(\"Starting model deployment: {}\".format(model_path))\n\
        \        response = client.deploy_model(model_name=model_path)\n        response.result()\
        \ # Wait for operation to complete\n        logging.info(\"Deployment completed\"\
        )\n    else:\n         logging.info(\"Model already deployed\")\n\nimport\
        \ argparse\n_parser = argparse.ArgumentParser(prog='Automl deploy model',\
        \ description='')\n_parser.add_argument(\"--model-path\", dest=\"model_path\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = automl_deploy_model(**_parsed_args)\n\
        \nif not hasattr(_outputs, '__getitem__') or isinstance(_outputs, str):\n\
        \    _outputs = [_outputs]\n\n_output_serializers = [\n    \n]\n\nimport os\n\
        for idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
        \    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n\
        \        f.write(_output_serializers[idx](_outputs[idx]))\n"
      env:
      - name: GOOGLE_APPLICATION_CREDENTIALS
        value: /secret/gcp-credentials/user-gcp-sa.json
      - name: CLOUDSDK_AUTH_CREDENTIAL_FILE_OVERRIDE
        value: /secret/gcp-credentials/user-gcp-sa.json
      image: gcr.io/jk-caip/lab_301_components:latest
      volumeMounts:
      - mountPath: /secret/gcp-credentials
        name: gcp-credentials-user-gcp-sa
    inputs:
      parameters:
      - name: automl-create-model-for-tables-model-path
    metadata:
      annotations:
        pipelines.kubeflow.org/component_spec: '{"inputs": [{"name": "model_path",
          "type": "String"}], "name": "Automl deploy model", "outputs": []}'
    name: automl-deploy-model
    volumes:
    - name: gcp-credentials-user-gcp-sa
      secret:
        secretName: user-gcp-sa
  - container:
      args:
      - --dataset-path
      - '{{inputs.parameters.automl-create-dataset-for-tables-dataset-path}}'
      - --input-uri
      - '{{inputs.parameters.bq-query-table-uri}}'
      - '----output-paths'
      - /tmp/outputs/dataset_path/data
      command:
      - python3
      - -u
      - -c
      - "from typing import NamedTuple\n\ndef automl_import_data_from_bigquery(\n\
        \    dataset_path,\n    input_uri: str,\n    retry=None, #=google.api_core.gapic_v1.method.DEFAULT,\n\
        \    timeout=None, #=google.api_core.gapic_v1.method.DEFAULT,\n    metadata:\
        \ dict = None,\n) -> NamedTuple('Outputs', [('dataset_path', str)]):\n   \
        \ import sys\n    import subprocess\n    subprocess.run([sys.executable, '-m',\
        \ 'pip', 'install', 'google-cloud-automl==0.4.0', '--quiet', '--no-warn-script-location'],\
        \ env={'PIP_DISABLE_PIP_VERSION_CHECK': '1'}, check=True)\n\n    import google\n\
        \    from google.cloud import automl\n    client = automl.AutoMlClient()\n\
        \    input_config = {\n        'bigquery_source': {\n            'input_uri':\
        \ input_uri,\n        },\n    }\n    response = client.import_data(\n    \
        \    dataset_path,\n        input_config,\n        retry or google.api_core.gapic_v1.method.DEFAULT,\n\
        \        timeout or google.api_core.gapic_v1.method.DEFAULT,\n        metadata,\n\
        \    )\n    result = response.result()\n    print(result)\n    metadata =\
        \ response.metadata\n    print(metadata)\n    return (dataset_path)\n\nimport\
        \ json\nimport argparse\n_missing_arg = object()\n_parser = argparse.ArgumentParser(prog='Automl\
        \ import data from bigquery', description='')\n_parser.add_argument(\"--dataset-path\"\
        , dest=\"dataset_path\", type=str, required=True, default=_missing_arg)\n\
        _parser.add_argument(\"--input-uri\", dest=\"input_uri\", type=str, required=True,\
        \ default=_missing_arg)\n_parser.add_argument(\"--retry\", dest=\"retry\"\
        , type=str, required=False, default=_missing_arg)\n_parser.add_argument(\"\
        --timeout\", dest=\"timeout\", type=str, required=False, default=_missing_arg)\n\
        _parser.add_argument(\"--metadata\", dest=\"metadata\", type=json.loads, required=False,\
        \ default=_missing_arg)\n_parser.add_argument(\"----output-paths\", dest=\"\
        _output_paths\", type=str, nargs=1)\n_parsed_args = {k: v for k, v in vars(_parser.parse_args()).items()\
        \ if v is not _missing_arg}\n_output_files = _parsed_args.pop(\"_output_paths\"\
        , [])\n\n_outputs = automl_import_data_from_bigquery(**_parsed_args)\n\nif\
        \ not hasattr(_outputs, '__getitem__') or isinstance(_outputs, str):\n   \
        \ _outputs = [_outputs]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
        \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
        \        pass\n    with open(output_file, 'w') as f:\n        f.write(str(_outputs[idx]))\n"
      env:
      - name: GOOGLE_APPLICATION_CREDENTIALS
        value: /secret/gcp-credentials/user-gcp-sa.json
      - name: CLOUDSDK_AUTH_CREDENTIAL_FILE_OVERRIDE
        value: /secret/gcp-credentials/user-gcp-sa.json
      image: python:3.7
      volumeMounts:
      - mountPath: /secret/gcp-credentials
        name: gcp-credentials-user-gcp-sa
    inputs:
      parameters:
      - name: automl-create-dataset-for-tables-dataset-path
      - name: bq-query-table-uri
    metadata:
      annotations:
        pipelines.kubeflow.org/component_spec: '{"inputs": [{"name": "dataset_path"},
          {"name": "input_uri", "type": "String"}, {"name": "retry", "optional": true},
          {"name": "timeout", "optional": true}, {"name": "metadata", "optional":
          true, "type": "JsonObject"}], "name": "Automl import data from bigquery",
          "outputs": [{"name": "dataset_path", "type": "String"}]}'
    name: automl-import-data-from-bigquery
    outputs:
      artifacts:
      - name: automl-import-data-from-bigquery-dataset-path
        path: /tmp/outputs/dataset_path/data
      parameters:
      - name: automl-import-data-from-bigquery-dataset-path
        valueFrom:
          path: /tmp/outputs/dataset_path/data
    volumes:
    - name: gcp-credentials-user-gcp-sa
      secret:
        secretName: user-gcp-sa
  - container:
      args:
      - --model-path
      - '{{inputs.parameters.automl-create-model-for-tables-model-path}}'
      - --primary-metric
      - '{{inputs.parameters.primary-metric}}'
      - '----output-paths'
      - /tmp/outputs/primary_metric/data
      - /tmp/outputs/primary_metric_value/data
      command:
      - python3
      - -u
      - -c
      - "from typing import NamedTuple\n\ndef automl_log_regression_metrics(model_path:\
        \ str,\n               primary_metric:str) -> NamedTuple('Outputs', [('primary_metric',\
        \ str), ('primary_metric_value', float)]):\n\n    import logging\n    import\
        \ json\n    from google.cloud import automl_v1beta1 as automl\n\n    logging.basicConfig(level=logging.INFO)\n\
        \    client = automl.TablesClient()\n\n    # Retrieve evaluation metrics\n\
        \    for evaluation in client.list_model_evaluations(model_name=model_path):\n\
        \        if evaluation.regression_evaluation_metrics.ListFields():\n     \
        \       evaluation_metrics = evaluation.regression_evaluation_metrics    \
        \  \n    primary_metric_value = getattr(evaluation_metrics, primary_metric)\n\
        \n    # Write the primary metric as a KFP pipeline metric\n    metrics = {\n\
        \        'metrics': [{\n            'name': primary_metric.replace('_', '-'),\n\
        \            'numberValue': primary_metric_value\n        }]\n    }\n    with\
        \ open('/mlpipeline-metrics.json', 'w') as f:\n       json.dump(metrics, f)\n\
        \n    return (primary_metric, primary_metric_value)\n\ndef _serialize_str(str_value:\
        \ str) -> str:\n    if not isinstance(str_value, str):\n        raise TypeError('Value\
        \ \"{}\" has type \"{}\" instead of str.'.format(str(str_value), str(type(str_value))))\n\
        \    return str_value\n\ndef _serialize_float(float_value: float) -> str:\n\
        \    if isinstance(float_value, str):\n        return float_value\n    if\
        \ not isinstance(float_value, (float, int)):\n        raise TypeError('Value\
        \ \"{}\" has type \"{}\" instead of float.'.format(str(float_value), str(type(float_value))))\n\
        \    return str(float_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Automl\
        \ log regression metrics', description='')\n_parser.add_argument(\"--model-path\"\
        , dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--primary-metric\", dest=\"primary_metric\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
        , dest=\"_output_paths\", type=str, nargs=2)\n_parsed_args = vars(_parser.parse_args())\n\
        _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = automl_log_regression_metrics(**_parsed_args)\n\
        \nif not hasattr(_outputs, '__getitem__') or isinstance(_outputs, str):\n\
        \    _outputs = [_outputs]\n\n_output_serializers = [\n    _serialize_str,\n\
        \    _serialize_float\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
        \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
        \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
      env:
      - name: GOOGLE_APPLICATION_CREDENTIALS
        value: /secret/gcp-credentials/user-gcp-sa.json
      - name: CLOUDSDK_AUTH_CREDENTIAL_FILE_OVERRIDE
        value: /secret/gcp-credentials/user-gcp-sa.json
      image: gcr.io/jk-caip/lab_301_components:latest
      volumeMounts:
      - mountPath: /secret/gcp-credentials
        name: gcp-credentials-user-gcp-sa
    inputs:
      parameters:
      - name: automl-create-model-for-tables-model-path
      - name: primary-metric
    metadata:
      annotations:
        pipelines.kubeflow.org/component_spec: '{"inputs": [{"name": "model_path",
          "type": "String"}, {"name": "primary_metric", "type": "String"}], "name":
          "Automl log regression metrics", "outputs": [{"name": "primary_metric",
          "type": "String"}, {"name": "primary_metric_value", "type": "Float"}]}'
    name: automl-log-regression-metrics
    outputs:
      artifacts:
      - name: automl-log-regression-metrics-primary-metric
        path: /tmp/outputs/primary_metric/data
      - name: automl-log-regression-metrics-primary-metric-value
        path: /tmp/outputs/primary_metric_value/data
      parameters:
      - name: automl-log-regression-metrics-primary-metric-value
        valueFrom:
          path: /tmp/outputs/primary_metric_value/data
    volumes:
    - name: gcp-credentials-user-gcp-sa
      secret:
        secretName: user-gcp-sa
  - container:
      args:
      - --dataset-path
      - '{{inputs.parameters.automl-import-data-from-bigquery-dataset-path}}'
      - --target-column-name
      - '{{inputs.parameters.target-column-name}}'
      - --table-index
      - '0'
      - '----output-paths'
      - /tmp/outputs/target_column_path/data
      - /tmp/outputs/feature_column_paths/data
      command:
      - python3
      - -u
      - -c
      - "from typing import NamedTuple\n\ndef automl_split_dataset_table_column_names(\n\
        \    dataset_path: str,\n    target_column_name: str,\n    table_index: int\
        \ = 0,\n) -> NamedTuple('Outputs', [('target_column_path', str), ('feature_column_paths',\
        \ list)]):\n    import sys\n    import subprocess\n    subprocess.run([sys.executable,\
        \ '-m', 'pip', 'install', 'google-cloud-automl==0.4.0', '--quiet', '--no-warn-script-location'],\
        \ env={'PIP_DISABLE_PIP_VERSION_CHECK': '1'}, check=True)\n\n    from google.cloud\
        \ import automl\n    client = automl.AutoMlClient()\n    list_table_specs_response\
        \ = client.list_table_specs(dataset_path)\n    table_specs = [s for s in list_table_specs_response]\n\
        \    print('table_specs=')\n    print(table_specs)\n    table_spec_name =\
        \ table_specs[table_index].name\n\n    list_column_specs_response = client.list_column_specs(table_spec_name)\n\
        \    column_specs = [s for s in list_column_specs_response]\n    print('column_specs=')\n\
        \    print(column_specs)\n\n    target_column_spec = [s for s in column_specs\
        \ if s.display_name == target_column_name][0]\n    feature_column_specs =\
        \ [s for s in column_specs if s.display_name != target_column_name]\n    feature_column_names\
        \ = [s.name for s in feature_column_specs]\n\n    import json\n    return\
        \ (target_column_spec.name, json.dumps(feature_column_names))\n\nimport argparse\n\
        _missing_arg = object()\n_parser = argparse.ArgumentParser(prog='Automl split\
        \ dataset table column names', description='')\n_parser.add_argument(\"--dataset-path\"\
        , dest=\"dataset_path\", type=str, required=True, default=_missing_arg)\n\
        _parser.add_argument(\"--target-column-name\", dest=\"target_column_name\"\
        , type=str, required=True, default=_missing_arg)\n_parser.add_argument(\"\
        --table-index\", dest=\"table_index\", type=int, required=False, default=_missing_arg)\n\
        _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str,\
        \ nargs=2)\n_parsed_args = {k: v for k, v in vars(_parser.parse_args()).items()\
        \ if v is not _missing_arg}\n_output_files = _parsed_args.pop(\"_output_paths\"\
        , [])\n\n_outputs = automl_split_dataset_table_column_names(**_parsed_args)\n\
        \nif not hasattr(_outputs, '__getitem__') or isinstance(_outputs, str):\n\
        \    _outputs = [_outputs]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
        \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
        \        pass\n    with open(output_file, 'w') as f:\n        f.write(str(_outputs[idx]))\n"
      env:
      - name: GOOGLE_APPLICATION_CREDENTIALS
        value: /secret/gcp-credentials/user-gcp-sa.json
      - name: CLOUDSDK_AUTH_CREDENTIAL_FILE_OVERRIDE
        value: /secret/gcp-credentials/user-gcp-sa.json
      image: python:3.7
      volumeMounts:
      - mountPath: /secret/gcp-credentials
        name: gcp-credentials-user-gcp-sa
    inputs:
      parameters:
      - name: automl-import-data-from-bigquery-dataset-path
      - name: target-column-name
    metadata:
      annotations:
        pipelines.kubeflow.org/component_spec: '{"inputs": [{"name": "dataset_path",
          "type": "String"}, {"name": "target_column_name", "type": "String"}, {"default":
          "0", "name": "table_index", "optional": true, "type": "Integer"}], "name":
          "Automl split dataset table column names", "outputs": [{"name": "target_column_path",
          "type": "String"}, {"name": "feature_column_paths", "type": "JsonArray"}]}'
    name: automl-split-dataset-table-column-names
    outputs:
      artifacts:
      - name: automl-split-dataset-table-column-names-feature-column-paths
        path: /tmp/outputs/feature_column_paths/data
      - name: automl-split-dataset-table-column-names-target-column-path
        path: /tmp/outputs/target_column_path/data
      parameters:
      - name: automl-split-dataset-table-column-names-feature-column-paths
        valueFrom:
          path: /tmp/outputs/feature_column_paths/data
      - name: automl-split-dataset-table-column-names-target-column-path
        valueFrom:
          path: /tmp/outputs/target_column_path/data
    volumes:
    - name: gcp-credentials-user-gcp-sa
      secret:
        secretName: user-gcp-sa
  - container:
      args:
      - --query
      - '{{inputs.parameters.feature-engineering-query}}'
      - --project-id
      - '{{inputs.parameters.project-id}}'
      - --dataset-id
      - '{{inputs.parameters.features-dataset-id}}'
      - --table-id
      - '{{inputs.parameters.features-table-id}}'
      - --location
      - '{{inputs.parameters.features-dataset-location}}'
      - '----output-paths'
      - /tmp/outputs/table_uri/data
      - /tmp/outputs/job_id/data
      command:
      - python3
      - -u
      - -c
      - "from typing import NamedTuple\n\ndef bq_query(query: str, \n            \
        \ project_id:str, \n             dataset_id: str, \n             table_id:\
        \ str, \n             location: str) -> NamedTuple('Outputs', [('table_uri',\
        \ str), ('job_id', str)]):\n\n    from google.cloud import bigquery\n    from\
        \ google.api_core import exceptions\n    import logging\n    import os\n \
        \   import uuid\n\n    logging.basicConfig(level=logging.INFO)\n\n    client\
        \ = bigquery.Client(project=project_id, location=location)\n\n    job_config\
        \ = bigquery.QueryJobConfig()\n    job_config.create_disposition = bigquery.job.CreateDisposition.CREATE_IF_NEEDED\n\
        \    job_config.write_disposition = bigquery.job.WriteDisposition.WRITE_TRUNCATE\n\
        \    job_id = 'query_' + os.environ.get('KFP_POD_NAME', uuid.uuid1().hex)\n\
        \n    dataset_ref = client.dataset(dataset_id)\n    try:\n        dataset\
        \ = client.get_dataset(dataset_ref)\n    except exceptions.NotFound:\n   \
        \     dataset = bigquery.Dataset(dataset_ref)\n        dataset.location =\
        \ location\n        logging.info('Creating dataset {}'.format(dataset_id))\n\
        \        client.create_dataset(dataset)\n\n    table_id = table_id if table_id\
        \ else job_id\n    table_ref = dataset_ref.table(table_id)\n    job_config.destination\
        \ = table_ref\n    logging.info('Submitting the job {}'.format(job_id))\n\
        \    query_job = client.query(query, job_config, job_id=job_id)\n    query_job.result()\
        \ # Wait for query to finish\n\n    table_uri = 'bq://{}.{}.{}'.format(project_id,\
        \ dataset_id, table_id)\n\n    return (table_uri, job_id)\n\ndef _serialize_str(str_value:\
        \ str) -> str:\n    if not isinstance(str_value, str):\n        raise TypeError('Value\
        \ \"{}\" has type \"{}\" instead of str.'.format(str(str_value), str(type(str_value))))\n\
        \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Bq\
        \ query', description='')\n_parser.add_argument(\"--query\", dest=\"query\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --project-id\", dest=\"project_id\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dataset-id\", dest=\"dataset_id\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--table-id\", dest=\"\
        table_id\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --location\", dest=\"location\", type=str, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str,\
        \ nargs=2)\n_parsed_args = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"\
        _output_paths\", [])\n\n_outputs = bq_query(**_parsed_args)\n\nif not hasattr(_outputs,\
        \ '__getitem__') or isinstance(_outputs, str):\n    _outputs = [_outputs]\n\
        \n_output_serializers = [\n    _serialize_str,\n    _serialize_str\n]\n\n\
        import os\nfor idx, output_file in enumerate(_output_files):\n    try:\n \
        \       os.makedirs(os.path.dirname(output_file))\n    except OSError:\n \
        \       pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
      env:
      - name: GOOGLE_APPLICATION_CREDENTIALS
        value: /secret/gcp-credentials/user-gcp-sa.json
      - name: CLOUDSDK_AUTH_CREDENTIAL_FILE_OVERRIDE
        value: /secret/gcp-credentials/user-gcp-sa.json
      image: gcr.io/jk-caip/lab_301_components:latest
      volumeMounts:
      - mountPath: /secret/gcp-credentials
        name: gcp-credentials-user-gcp-sa
    inputs:
      parameters:
      - name: feature-engineering-query
      - name: features-dataset-id
      - name: features-dataset-location
      - name: features-table-id
      - name: project-id
    metadata:
      annotations:
        pipelines.kubeflow.org/component_spec: '{"inputs": [{"name": "query", "type":
          "String"}, {"name": "project_id", "type": "String"}, {"name": "dataset_id",
          "type": "String"}, {"name": "table_id", "type": "String"}, {"name": "location",
          "type": "String"}], "name": "Bq query", "outputs": [{"name": "table_uri",
          "type": "String"}, {"name": "job_id", "type": "String"}]}'
    name: bq-query
    outputs:
      artifacts:
      - name: bq-query-job-id
        path: /tmp/outputs/job_id/data
      - name: bq-query-table-uri
        path: /tmp/outputs/table_uri/data
      parameters:
      - name: bq-query-table-uri
        valueFrom:
          path: /tmp/outputs/table_uri/data
    volumes:
    - name: gcp-credentials-user-gcp-sa
      secret:
        secretName: user-gcp-sa
  - dag:
      tasks:
      - arguments:
          parameters:
          - name: aml-compute-region
            value: '{{inputs.parameters.aml-compute-region}}'
          - name: aml-dataset-name
            value: '{{inputs.parameters.aml-dataset-name}}'
          - name: project-id
            value: '{{inputs.parameters.project-id}}'
        name: automl-create-dataset-for-tables
        template: automl-create-dataset-for-tables
      - arguments:
          parameters:
          - name: aml-compute-region
            value: '{{inputs.parameters.aml-compute-region}}'
          - name: aml-model-name
            value: '{{inputs.parameters.aml-model-name}}'
          - name: automl-create-dataset-for-tables-dataset-id
            value: '{{tasks.automl-create-dataset-for-tables.outputs.parameters.automl-create-dataset-for-tables-dataset-id}}'
          - name: automl-split-dataset-table-column-names-feature-column-paths
            value: '{{tasks.automl-split-dataset-table-column-names.outputs.parameters.automl-split-dataset-table-column-names-feature-column-paths}}'
          - name: automl-split-dataset-table-column-names-target-column-path
            value: '{{tasks.automl-split-dataset-table-column-names.outputs.parameters.automl-split-dataset-table-column-names-target-column-path}}'
          - name: optimization-objective
            value: '{{inputs.parameters.optimization-objective}}'
          - name: project-id
            value: '{{inputs.parameters.project-id}}'
          - name: train-budget
            value: '{{inputs.parameters.train-budget}}'
        dependencies:
        - automl-create-dataset-for-tables
        - automl-split-dataset-table-column-names
        name: automl-create-model-for-tables
        template: automl-create-model-for-tables
      - arguments:
          parameters:
          - name: automl-create-dataset-for-tables-dataset-path
            value: '{{tasks.automl-create-dataset-for-tables.outputs.parameters.automl-create-dataset-for-tables-dataset-path}}'
          - name: bq-query-table-uri
            value: '{{tasks.bq-query.outputs.parameters.bq-query-table-uri}}'
        dependencies:
        - automl-create-dataset-for-tables
        - bq-query
        name: automl-import-data-from-bigquery
        template: automl-import-data-from-bigquery
      - arguments:
          parameters:
          - name: automl-create-model-for-tables-model-path
            value: '{{tasks.automl-create-model-for-tables.outputs.parameters.automl-create-model-for-tables-model-path}}'
          - name: primary-metric
            value: '{{inputs.parameters.primary-metric}}'
        dependencies:
        - automl-create-model-for-tables
        name: automl-log-regression-metrics
        template: automl-log-regression-metrics
      - arguments:
          parameters:
          - name: automl-import-data-from-bigquery-dataset-path
            value: '{{tasks.automl-import-data-from-bigquery.outputs.parameters.automl-import-data-from-bigquery-dataset-path}}'
          - name: target-column-name
            value: '{{inputs.parameters.target-column-name}}'
        dependencies:
        - automl-import-data-from-bigquery
        name: automl-split-dataset-table-column-names
        template: automl-split-dataset-table-column-names
      - arguments:
          parameters:
          - name: feature-engineering-query
            value: '{{inputs.parameters.feature-engineering-query}}'
          - name: features-dataset-id
            value: '{{inputs.parameters.features-dataset-id}}'
          - name: features-dataset-location
            value: '{{inputs.parameters.features-dataset-location}}'
          - name: features-table-id
            value: '{{inputs.parameters.features-table-id}}'
          - name: project-id
            value: '{{inputs.parameters.project-id}}'
        name: bq-query
        template: bq-query
      - arguments:
          parameters:
          - name: automl-create-model-for-tables-model-path
            value: '{{tasks.automl-create-model-for-tables.outputs.parameters.automl-create-model-for-tables-model-path}}'
        dependencies:
        - automl-create-model-for-tables
        - automl-log-regression-metrics
        name: condition-1
        template: condition-1
        when: '{{tasks.automl-log-regression-metrics.outputs.parameters.automl-log-regression-metrics-primary-metric-value}}
          < {{inputs.parameters.deployment-threshold}}'
    inputs:
      parameters:
      - name: aml-compute-region
      - name: aml-dataset-name
      - name: aml-model-name
      - name: deployment-threshold
      - name: feature-engineering-query
      - name: features-dataset-id
      - name: features-dataset-location
      - name: features-table-id
      - name: optimization-objective
      - name: primary-metric
      - name: project-id
      - name: target-column-name
      - name: train-budget
    name: clv-training
  - dag:
      tasks:
      - arguments:
          parameters:
          - name: automl-create-model-for-tables-model-path
            value: '{{inputs.parameters.automl-create-model-for-tables-model-path}}'
        name: automl-deploy-model
        template: automl-deploy-model
    inputs:
      parameters:
      - name: automl-create-model-for-tables-model-path
    name: condition-1
