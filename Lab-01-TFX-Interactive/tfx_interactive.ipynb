{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFX Interactive",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "qMj8ORjK27p9"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jarokaz/mlops-miniworkshop/blob/master/Lab-01-TFX-Interactive/tfx_interactive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMj8ORjK27p9",
        "colab_type": "text"
      },
      "source": [
        "##### Copyright &copy; 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JwKPOmN2-15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23R0Z9RojXYW",
        "colab_type": "text"
      },
      "source": [
        "# Exploring data with TFDV and experimenting with TFX components in an interactive context\n",
        "\n",
        "This notebook demonstrates how to use Jupyter notebooks for TFX iterative development. \n",
        "\n",
        "Working in an interactive notebook is useful when doing initial data exploriation, experimenting with models, and designing ML pipelines. You should be aware that there are differences in the way interactive notebooks are orchestrated, and how they access metadata artifacts.\n",
        "\n",
        "In a production deployment of TFX you will use an orchestrator such as Kubeflow Pipelines, or Apache Beam.  In an interactive notebook the notebook itself is the orchestrator, running each TFX component as you execute the notebook cells.\n",
        "\n",
        "In a production deployment of TFX you will access metadata through the ML Metadata (MLMD) API.  MLMD stores metadata properties in a database such as MySQL, and stores the metadata payloads in a persistent store such as Google Cloud Storage.  In an interactive notebook, both properties and payloads are stored in the local file system of the Jupyter host.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GivNBNYjb3b",
        "colab_type": "text"
      },
      "source": [
        "## Setup\n",
        "First, you install the necessary packages, download data, import modules and set up paths.\n",
        "\n",
        "### Install TFX and TensorFlow\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LwniiFz6hRr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRbSCw-U-h_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q -U \\\n",
        "  tfx==0.15.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-ePgV0Lj68Q",
        "colab_type": "text"
      },
      "source": [
        "### Import packages\n",
        "Import necessary packages, including standard TFDV and TFX component classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIqpWK9efviJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import base64\n",
        "import csv\n",
        "import json\n",
        "import os\n",
        "import requests\n",
        "import tempfile\n",
        "import urllib\n",
        "import pprint\n",
        "pp = pprint.PrettyPrinter()\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import tfx\n",
        "from tfx.components.evaluator.component import Evaluator\n",
        "from tfx.components.example_gen.csv_example_gen.component import CsvExampleGen\n",
        "from tfx.components.example_validator.component import ExampleValidator\n",
        "from tfx.components.model_validator.component import ModelValidator\n",
        "from tfx.components.pusher.component import Pusher\n",
        "from tfx.components.schema_gen.component import SchemaGen\n",
        "from tfx.components.statistics_gen.component import StatisticsGen\n",
        "from tfx.components.trainer.component import Trainer\n",
        "from tfx.components.transform.component import Transform\n",
        "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
        "from tfx.proto import evaluator_pb2\n",
        "from tfx.proto import pusher_pb2\n",
        "from tfx.proto import trainer_pb2\n",
        "from tfx.utils.dsl_utils import external_input\n",
        "from tfx.proto import example_gen_pb2\n",
        "\n",
        "from tensorflow_metadata.proto.v0 import anomalies_pb2\n",
        "from tensorflow_metadata.proto.v0 import schema_pb2\n",
        "from tensorflow_metadata.proto.v0 import statistics_pb2\n",
        "\n",
        "import tensorflow_transform as tft\n",
        "from tensorflow_transform import coders as tft_coders\n",
        "from tensorflow_transform.tf_metadata import dataset_schema\n",
        "from tensorflow_transform.tf_metadata import schema_utils\n",
        "\n",
        "import tensorflow_model_analysis as tfma\n",
        "import tensorflow_data_validation as tfdv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlhCYXop3vcd",
        "colab_type": "text"
      },
      "source": [
        "Check the versions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZY7Pnoxmoe8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('TensorFlow version: {}'.format(tf.__version__))\n",
        "print('TFX version: {}'.format(tfx.__version__))\n",
        "print('TensorFlow Data Validation version: {}'.format(tfdv.__version__))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2cMMAbSkGfX",
        "colab_type": "text"
      },
      "source": [
        "### Download example data\n",
        "You are working with a variant of the [Online News Popularity](https://archive.ics.uci.edu/ml/datasets/online+news+popularity) dataset, which summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict how popular the article will be on social networks. Specifically, in the original dataset the objective was to predict the number of times each article will be shared on social networks. In this variant, the goal is to predict the article's popularity percentile. For example, if the model predicts a score of 0.7, then it means it expects the article to be shared more than 70% of all articles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BywX6OUEhAqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download the example data.\n",
        "GCS_DATA_PATH = 'https://storage.googleapis.com/workshop-datasets/online_news/{}/data.csv'\n",
        "_data_root = tempfile.mkdtemp(prefix='tfx-data')\n",
        "for folder in ['train', 'eval', 'serving']:\n",
        "    os.makedirs(os.path.join(_data_root, folder))\n",
        "    _data_filepath = os.path.join(_data_root, folder, \"data.csv\")\n",
        "    urllib.request.urlretrieve(GCS_DATA_PATH.format(folder), _data_filepath)\n",
        "_train_data_path = os.path.join(_data_root, 'train', 'data.csv')\n",
        "_serving_data_path = os.path.join(_data_root, 'serving', 'data.csv')\n",
        "_eval_data_path = os.path.join(_data_root, 'eval', 'data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5RjduOX4us-",
        "colab_type": "text"
      },
      "source": [
        "Take a quick look at the CSV file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hqn4wST2Bex5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!head {_train_data_path}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5PKyfOLHD1s",
        "colab_type": "text"
      },
      "source": [
        "# Explore data\n",
        "\n",
        "## Compute and visualize statistics\n",
        "\n",
        "First we'll use [`tfdv.generate_statistics_from_csv`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/generate_statistics_from_csv) to compute statistics for our training data. (ignore the snappy warnings)\n",
        "\n",
        "TFDV can compute descriptive [statistics](https://github.com/tensorflow/metadata/blob/v0.6.0/tensorflow_metadata/proto/v0/statistics.proto) that provide a quick overview of the data in terms of the features that are present and the shapes of their value distributions.\n",
        "\n",
        "Internally, TFDV uses [Apache Beam](https://beam.apache.org/)'s data-parallel processing framework to scale the computation of statistics over large datasets. For applications that wish to integrate deeper with TFDV (e.g., attach statistics generation at the end of a data-generation pipeline), the API also exposes a Beam PTransform for statistics generation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VqfEu1SHWqp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_stats = tfdv.generate_statistics_from_csv(\n",
        "    data_location=_train_data_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lx4VnGmHk99",
        "colab_type": "text"
      },
      "source": [
        "Now let's use [`tfdv.visualize_statistics`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/visualize_statistics), which uses [Facets](https://pair-code.github.io/facets/) to create a succinct visualization of our training data:\n",
        "\n",
        "* Notice that numeric features and catagorical features are visualized separately, and that charts are displayed showing the distributions for each feature.\n",
        "* Notice that features with missing or zero values display a percentage in red as a visual indicator that there may be issues with examples in those features.  The percentage is the percentage of examples that have missing or zero values for that feature.\n",
        "* Notice that there are no examples with values for `pickup_census_tract`.  This is an opportunity for dimensionality reduction!\n",
        "* Try clicking \"expand\" above the charts to change the display\n",
        "* Try hovering over bars in the charts to display bucket ranges and counts\n",
        "* Try switching between the log and linear scales, and notice how the log scale reveals much more detail about the `payment_type` categorical feature\n",
        "* Try selecting \"quantiles\" from the \"Chart to show\" menu, and hover over the markers to show the quantile percentages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Splsp3X6Hp0F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfdv.visualize_statistics(train_stats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZbXlUR_Hv_l",
        "colab_type": "text"
      },
      "source": [
        "## Infer a schema\n",
        "\n",
        "Now let's use [`tfdv.infer_schema`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/infer_schema) to create a schema for our data.  A schema defines constraints for the data that are relevant for ML. Example constraints include the data type of each feature, whether it's numerical or categorical, or the frequency of its presence in the data.  For categorical features the schema also defines the domain - the list of acceptable values.  Since writing a schema can be a tedious task, especially for datasets with lots of features, TFDV provides a method to generate an initial version of the schema based on the descriptive statistics.\n",
        "\n",
        "Getting the schema right is important because the rest of our production pipeline will be relying on the schema that TFDV generates to be correct.  The schema also provides documentation for the data, and so is useful when different developers work on the same data.  Let's use [`tfdv.display_schema`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/display_schema) to display the inferred schema so that we can review it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEW1s3h5H1MM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "schema = tfdv.infer_schema(statistics=train_stats)\n",
        "tfdv.display_schema(schema=schema)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eteAuvz_IC5E",
        "colab_type": "text"
      },
      "source": [
        "## Check evaluation data for errors\n",
        "\n",
        "So far we've only been looking at the training data.  It's important that our evaluation data is consistent with our training data, including that it uses the same schema.  It's also important that the evaluation data includes examples of roughly the same ranges of values for our numerical features as our training data, so that our coverage of the loss surface during evaluation is roughly the same as during training.  The same is true for categorical features.  Otherwise, we may have training issues that are not identified during evaluation, because we didn't evaluate part of our loss surface.\n",
        "\n",
        "* Notice that each feature now includes statistics for both the training and evaluation datasets.\n",
        "* Notice that the charts now have both the training and evaluation datasets overlaid, making it easy to compare them.\n",
        "* Notice that the charts now include a percentages view, which can be combined with log or the default linear scales.\n",
        "* Notice that some features are significantly different for the training versus the evaluation datasets, in particular check the mean and median.  Will that cause problems?\n",
        "* Click expand on the Numeric Features chart, and select the log scale.  Review the `n_hrefs` feature, and notice the difference in the max.  Will evaluation miss parts of the loss surface?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhxleJ1qIH8u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_stats = tfdv.generate_statistics_from_csv(\n",
        "    data_location=_eval_data_path)\n",
        "\n",
        "tfdv.visualize_statistics(lhs_statistics=eval_stats, rhs_statistics=train_stats,\n",
        "                          lhs_name='EVAL_DATASET', rhs_name='TRAIN_DATASET')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ3XIHp9Ifhc",
        "colab_type": "text"
      },
      "source": [
        "## Check for evaluation anomalies\n",
        "\n",
        "Does our evaluation dataset match the schema from our training dataset?  This is especially important for categorical features, where we want to identify the range of acceptable values.\n",
        "\n",
        "Key Point: What would happen if we tried to evaluate using data with categorical feature values that were not in our training dataset?  What about numeric features that are outside the ranges in our training dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOCFzr-xIkMA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check eval data for errors by validating the eval data stats using the previously inferred schema.\n",
        "\n",
        "anomalies = tfdv.validate_statistics(statistics=eval_stats, schema=schema)\n",
        "tfdv.display_anomalies(anomalies)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKxUZjpvIo3U",
        "colab_type": "text"
      },
      "source": [
        "## Fix evaluation anomalies in the schema\n",
        "\n",
        "Oops!  It looks like we have some new values for `data_channel` in our evaluation data, that we didn't have in our training data (what a surprise!). This should be considered an anomaly, but what we decide to do about it depends on our domain knowledge of the data. If an anomaly truly indicates a data error, then the underlying data should be fixed.  Otherwise, we can simply update the schema to include the values in the eval dataset.\n",
        "\n",
        "Key Point: How would our evaluation results be affected if we did not fix this problem?\n",
        "\n",
        "Unless we change our evaluation dataset we can't fix everything, but we can fix things in the schema that we're comfortable accepting.  That includes relaxing our view of what is and what is not an anomaly for particular features, as well as updating our schema to include missing values for categorical features.  TFDV has enabled us to discover what we need to fix.\n",
        "\n",
        "Let's make the fix now, and then review one more time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZC9gl_qIwq0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Relax the minimum fraction of values that must come\n",
        "# from the domain for feature data_channel.\n",
        "data_channel = tfdv.get_feature(schema, 'data_channel')\n",
        "data_channel.distribution_constraints.min_domain_mass = 1.0\n",
        "\n",
        "# Add new value to the domain of feature data_channel.\n",
        "data_channel_domain = tfdv.get_domain(schema, 'data_channel')\n",
        "data_channel_domain.value.append('Fun')\n",
        "\n",
        "# Validate eval stats after updating the schema \n",
        "updated_anomalies = tfdv.validate_statistics(eval_stats, schema)\n",
        "tfdv.display_anomalies(updated_anomalies)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEeAoWHEJKa6",
        "colab_type": "text"
      },
      "source": [
        "## Schema Environments\n",
        "\n",
        "We also split off a 'serving' dataset for this example, so we should check that too.  By default all datasets in a pipeline should use the same schema, but there are often exceptions. For example, in supervised learning we need to include labels in our dataset, but when we serve the model for inference the labels will not be included. In some cases introducing slight schema variations is necessary.\n",
        "\n",
        "**Environments** can be used to express such requirements. In particular, features in schema can be associated with a set of environments using `default_environment`, `in_environment` and `not_in_environment`.\n",
        "\n",
        "For example, in this dataset the `n_shares_percentile` feature is included as the label for training, but it's missing in the serving data. Without environment specified, it will show up as an anomaly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqfEMw7iJJmy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "serving_stats = tfdv.generate_statistics_from_csv(_serving_data_path)\n",
        "serving_anomalies = tfdv.validate_statistics(serving_stats, schema)\n",
        "\n",
        "tfdv.display_anomalies(serving_anomalies)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqympJYZJc5f",
        "colab_type": "text"
      },
      "source": [
        "TDFV noticed that the `n_shares_percentile` column is missing in the serving set (as expected), and it also noticed that some features which should be floats are actually integers.\n",
        "It's very easy to be unaware of problems like that until model performance suffers, sometimes catastrophically. It may or may not be a significant issue, but in any case this should be cause for further investigation.\n",
        "\n",
        "In this case, we can safely convert integers to floats, so we want to tell TFDV to use our schema to infer the type. Let's do that now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kt0Mr26Jnsw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "options = tfdv.StatsOptions(schema=schema, infer_type_from_schema=True)\n",
        "serving_stats = tfdv.generate_statistics_from_csv(_serving_data_path,\n",
        "                                                  stats_options=options)\n",
        "serving_anomalies = tfdv.validate_statistics(serving_stats, schema)\n",
        "\n",
        "tfdv.display_anomalies(serving_anomalies)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyGamFa-KCEE",
        "colab_type": "text"
      },
      "source": [
        "## Check for drift and skew\n",
        "\n",
        "In addition to checking whether a dataset conforms to the expectations set in the schema, TFDV also provides functionalities to detect drift and skew.  TFDV performs this check by comparing the statistics of the different datasets based on the drift/skew comparators specified in the schema.\n",
        "\n",
        "### Drift\n",
        "\n",
        "Drift detection is supported for categorical features and between consecutive spans of data (i.e., between span N and span N+1), such as between different days of training data.  We express drift in terms of [L-infinity distance](https://en.wikipedia.org/wiki/Chebyshev_distance), and you can set the threshold distance so that you receive warnings when the drift is higher than is acceptable.  Setting the correct distance is typically an iterative process requiring domain knowledge and experimentation.\n",
        "\n",
        "### Skew\n",
        "\n",
        "TFDV can detect three different kinds of skew in your data - schema skew, feature skew, and distribution skew.\n",
        "\n",
        "#### Schema Skew\n",
        "\n",
        "Schema skew occurs when the training and serving data do not conform to the same schema. Both training and serving data are expected to adhere to the same schema. Any expected deviations between the two (such as the label feature being only present in the training data but not in serving) should be specified through environments field in the schema.\n",
        "\n",
        "#### Feature Skew\n",
        "\n",
        "Feature skew occurs when the feature values that a model trains on are different from the feature values that it sees at serving time. For example, this can happen when:\n",
        "\n",
        "* A data source that provides some feature values is modified between training and serving time\n",
        "* There is different logic for generating features between training and serving. For example, if you apply some transformation only in one of the two code paths.\n",
        "\n",
        "#### Distribution Skew\n",
        "\n",
        "Distribution skew occurs when the distribution of the training dataset is significantly different from the distribution of the serving dataset. One of the key causes for distribution skew is using different code or different data sources to generate the training dataset. Another reason is a faulty sampling mechanism that chooses a non-representative subsample of the serving data to train on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIPYhWkSKEk7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add skew comparator for 'weekday' feature.\n",
        "weekday = tfdv.get_feature(schema, 'weekday')\n",
        "weekday.skew_comparator.infinity_norm.threshold = 0.01\n",
        "\n",
        "# Add drift comparator for 'weekday' feature.\n",
        "weekday.drift_comparator.infinity_norm.threshold = 0.001\n",
        "\n",
        "skew_anomalies = tfdv.validate_statistics(train_stats, schema,\n",
        "                                          previous_statistics=eval_stats,\n",
        "                                          serving_statistics=serving_stats)\n",
        "\n",
        "tfdv.display_anomalies(skew_anomalies)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "072jEwpjKKfo",
        "colab_type": "text"
      },
      "source": [
        "## Freeze the schema\n",
        "\n",
        "Now that the schema has been reviewed and curated, we will store it in a file to reflect its \"frozen\" state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmGJU_W_KN-f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_output_dir = os.path.join(tempfile.mkdtemp(),\n",
        "                           'serving_model/online_news_simple')\n",
        "\n",
        "from google.protobuf import text_format\n",
        "\n",
        "tf.io.gfile.makedirs(_output_dir)\n",
        "schema_file = os.path.join(_output_dir, 'schema.pbtxt')\n",
        "tfdv.write_schema_text(schema, schema_file)\n",
        "\n",
        "!cat {schema_file}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PaBysolKgi5",
        "colab_type": "text"
      },
      "source": [
        "## When to use TFDV\n",
        "\n",
        "It's easy to think of TFDV as only applying to the start of your training pipeline, as we did here, but in fact it has many uses.  Here's a few more:\n",
        "\n",
        "* Validating new data for inference to make sure that we haven't suddenly started receiving bad features\n",
        "* Validating new data for inference to make sure that our model has trained on that part of the decision surface\n",
        "* Validating our data after we've transformed it and done feature engineering (probably using [TensorFlow Transform](https://www.tensorflow.org/tfx/transform/)) to make sure we haven't done something wrong"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdQWxfsVkzdJ",
        "colab_type": "text"
      },
      "source": [
        "# Run TFX Components Interactively\n",
        "\n",
        "---\n",
        "\n",
        "In the cells that follow you will construct TFX components and run each one interactively within the InteractiveContext to obtain `ExecutionResult` objects.  This mirrors the process of an orchestrator running components in a TFX DAG based on when the dependencies for each component are met.\n",
        "## Create InteractiveContex\n",
        "An interactive context is used to provide global context when running a TFX pipeline in a notebook without using a runner or orchestrator such as Apache Airflow or Kubeflow.  This style of development is only useful when developing the code for a pipeline, and cannot currently be used to deploy a working pipeline to production.\n",
        "\n",
        "Here, we create an InteractiveContext using default parameters. This will\n",
        "use a temporary directory with an ephemeral ML Metadata database instance.\n",
        "To use your own pipeline root or database, the optional properties\n",
        "`pipeline_root` and `metadata_connection_config` may be passed to\n",
        "InteractiveContext."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kA3aC4JYK90y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "context = InteractiveContext()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9fwt9gQk3BR",
        "colab_type": "text"
      },
      "source": [
        "### The ExampleGen Component\n",
        "In any ML development process the first step when starting code development is to ingest the training and test datasets.  The `ExampleGen` component brings data into the TFX pipeline.\n",
        "\n",
        "Let's create an ExampleGen component and run it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyXjuMt8f-9u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_data = external_input(_data_root)\n",
        "input_config = example_gen_pb2.Input(splits=[\n",
        "    example_gen_pb2.Input.Split(name='train', pattern='train/*'),\n",
        "    example_gen_pb2.Input.Split(name='eval', pattern='eval/*')\n",
        "])\n",
        "\n",
        "example_gen = CsvExampleGen(input=input_data, input_config=input_config)\n",
        "context.run(example_gen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SXc2OGnDWz5",
        "colab_type": "text"
      },
      "source": [
        "The component's outputs include 2 artifacts: the training examples and the eval examples. If we only specified a single input the component would split the data using the default split: 2/3 training, 1/3 eval:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoFaWckTy8pL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for artifact in example_gen.outputs['examples'].get():\n",
        "  print(artifact.split, artifact.uri)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfzF2WObDqs6",
        "colab_type": "text"
      },
      "source": [
        "Take a peek at the output training examples to see what they look like.\n",
        "\n",
        "1. Get the URI of the output artifact representing the training examples, which is a directory\n",
        "1. Get the list of files in this directory (all compressed TFRecord files), and create a `TFRecordDataset` to read these files\n",
        "1. Iterate over the first record and decode it using a `TFExampleDecoder` to check the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEHi7dFLzZWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_uri = example_gen.outputs['examples'].get()[0].uri\n",
        "tfrecord_filenames = [os.path.join(train_uri, name)\n",
        "                      for name in os.listdir(train_uri)]\n",
        "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
        "decoder = tfdv.TFExampleDecoder()\n",
        "for tfrecord in dataset.take(1):\n",
        "  serialized_example = tfrecord.numpy()\n",
        "  example = decoder.decode(serialized_example)\n",
        "  pp.pprint(example)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csM6BFhtk5Aa",
        "colab_type": "text"
      },
      "source": [
        "### The StatisticsGen Component\n",
        "\n",
        "The `StatisticsGen` component computes descriptive statistics for your dataset.  The statistics that it generates can be visualized for review, and are used for example validation and to infer a schema.\n",
        "\n",
        "In the previous section we showed how to generate and analyze statistics using TFDV directly. The StatisticsGen component is a wrapper around TFDV. It supports the workflows when you need to generate statistics as a step in an ML pipeline.\n",
        "\n",
        "Create a StatisticsGen component and run it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAscCCYWgA-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Computes statistics over data for visualization and example validation.\n",
        "statistics_gen = StatisticsGen(\n",
        "    examples=example_gen.outputs['examples'])\n",
        "context.run(statistics_gen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5LBVkeDEvZQ",
        "colab_type": "text"
      },
      "source": [
        "Again, let's take a peek at the output training artifact. Note that this time it is a TFRecord file containing a single record with a serialized `DatasetFeatureStatisticsList` protobuf:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHP1HKDc3EXY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_uri = statistics_gen.outputs['statistics'].get()[0].uri\n",
        "tfrecord_filenames = [os.path.join(train_uri, name)\n",
        "                      for name in os.listdir(train_uri)]\n",
        "dataset = tf.data.TFRecordDataset(tfrecord_filenames)\n",
        "for tfrecord in dataset.take(1):\n",
        "  serialized_example = tfrecord.numpy()\n",
        "  stats = statistics_pb2.DatasetFeatureStatisticsList()\n",
        "  stats.ParseFromString(serialized_example)\n",
        "stats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRNfT5_aFGJC",
        "colab_type": "text"
      },
      "source": [
        "As before we could use the `tfdv.visualize_statistics()` function to visualize the stats."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLKLTO9Nk60p",
        "colab_type": "text"
      },
      "source": [
        "### The SchemaGen Component\n",
        "\n",
        "The `SchemaGen` component generates a schema for your data based on the statistics from StatisticsGen.  It is a wrapper around TFDV functions we reviewed in the previous steps. In most cases you would generate the schema as during the EDA exercise. In rare cases you may need to generate the schema as part of an ML pipeline and this is the role of this component.\n",
        "\n",
        "Create a SchemaGen component and run it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s9hcxORhQDwJ",
        "colab": {}
      },
      "source": [
        "# Generates schema based on statistics files.\n",
        "infer_schema = SchemaGen(statistics=statistics_gen.outputs['statistics'])\n",
        "context.run(infer_schema)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdtU3u01FR-2",
        "colab_type": "text"
      },
      "source": [
        "The generated artifact is just a `schema.pbtxt` containing a text representation of a `schema_pb2.Schema` protobuf:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6-tgKi6A_gK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_uri = infer_schema.outputs['schema'].get()[0].uri\n",
        "schema_filename = os.path.join(train_uri, \"schema.pbtxt\")\n",
        "schema = tfx.utils.io_utils.parse_pbtxt_file(file_name=schema_filename,\n",
        "                                             message=schema_pb2.Schema())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaSgx5qIFelw",
        "colab_type": "text"
      },
      "source": [
        "It can be visualized using `tfdv.display_schema()` (we will look at this in more detail in a subsequent lab):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gycOsJIQFhi3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfdv.display_schema(schema)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1qcUuO9k9f8",
        "colab_type": "text"
      },
      "source": [
        "### The ExampleValidator Component\n",
        "\n",
        "The `ExampleValidator` performs anomaly detection, based on the statistics from StatisticsGen and the schema from SchemaGen.  It is a wrapper around TFDV functions reviewed before.\n",
        "\n",
        "Create an ExampleValidator component and run it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRlRUuGgiXks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Performs anomaly detection based on statistics and data schema.\n",
        "validate_stats = ExampleValidator(\n",
        "    statistics=statistics_gen.outputs['statistics'],\n",
        "    schema=infer_schema.outputs['schema'])\n",
        "context.run(validate_stats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWUDXqADFp5U",
        "colab_type": "text"
      },
      "source": [
        "The output artifact of the `ExampleValidator` is an `anomalies.pbtxt` file describing an `anomalies_pb2.Anomalies` protobuf:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMX0LCyHCKGH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_uri = validate_stats.outputs['anomalies'].get()[0].uri\n",
        "anomalies_filename = os.path.join(train_uri, \"anomalies.pbtxt\")\n",
        "anomalies = tfx.utils.io_utils.parse_pbtxt_file(\n",
        "    file_name=anomalies_filename,\n",
        "    message=anomalies_pb2.Anomalies())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-nMbzFbF22_",
        "colab_type": "text"
      },
      "source": [
        "This can be visualized using the `tfdv.display_anomalies()`. Since the component generated a new schema we will see the same anomaly as in the previous section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGfl7-8YF2Ej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfdv.display_anomalies(anomalies)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPViEz5RlA36",
        "colab_type": "text"
      },
      "source": [
        "### The Transform Component\n",
        "\n",
        "The `Transform` component performs data transformations and feature engineering.  The results include an input TensorFlow graph which is used during both training and serving to preprocess the data before training or inference.  This graph becomes part of the SavedModel that is the result of model training.  Since the same input graph is used for both training and serving, the preprocessing will always be the same, and only needs to be written once.\n",
        "\n",
        "The Transform component requires more code than many other components because of the arbitrary complexity of the feature engineering that you may need for the data and/or model that you're working with.  It requires code files to be available which define the processing needed.\n",
        "\n",
        "Define some constants and functions for both the `Transform` component and the `Trainer` component.  Define them in a Python module, in this case saved to disk using the `%%writefile` magic command since you are working in a notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvZ6OFHDG2fe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_constants_module_file = 'online_news_constants.py'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GpU9-JNXw-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile {_constants_module_file}\n",
        "\n",
        "DENSE_FLOAT_FEATURE_KEYS = [\n",
        "    \"timedelta\", \"n_tokens_title\", \"n_tokens_content\",\n",
        "    \"n_unique_tokens\", \"n_non_stop_words\", \"n_non_stop_unique_tokens\",\n",
        "    \"n_hrefs\", \"n_self_hrefs\", \"n_imgs\", \"n_videos\", \"average_token_length\",\n",
        "    \"n_keywords\", \"kw_min_min\", \"kw_max_min\", \"kw_avg_min\", \"kw_min_max\",\n",
        "    \"kw_max_max\", \"kw_avg_max\", \"kw_min_avg\", \"kw_max_avg\", \"kw_avg_avg\",\n",
        "    \"self_reference_min_shares\", \"self_reference_max_shares\",\n",
        "    \"self_reference_avg_shares\", \"is_weekend\", \"global_subjectivity\",\n",
        "    \"global_sentiment_polarity\", \"global_rate_positive_words\",\n",
        "    \"global_rate_negative_words\", \"rate_positive_words\", \"rate_negative_words\",\n",
        "    \"avg_positive_polarity\", \"min_positive_polarity\", \"max_positive_polarity\",\n",
        "    \"avg_negative_polarity\", \"min_negative_polarity\", \"max_negative_polarity\",\n",
        "    \"title_subjectivity\", \"title_sentiment_polarity\", \"abs_title_subjectivity\",\n",
        "    \"abs_title_sentiment_polarity\"]\n",
        "\n",
        "VOCAB_FEATURE_KEYS = [\"data_channel\"]\n",
        "\n",
        "BUCKET_FEATURE_KEYS = [\"LDA_00\", \"LDA_01\", \"LDA_02\", \"LDA_03\", \"LDA_04\"]\n",
        "\n",
        "CATEGORICAL_FEATURE_KEYS = [\"weekday\"]\n",
        "\n",
        "# Categorical features are assumed to each have a maximum value in the dataset.\n",
        "MAX_CATEGORICAL_FEATURE_VALUES = [6]\n",
        "\n",
        "#UNUSED: date, slug\n",
        "\n",
        "LABEL_KEY = \"n_shares_percentile\"\n",
        "VOCAB_SIZE = 10\n",
        "OOV_SIZE = 5\n",
        "FEATURE_BUCKET_COUNT = 10\n",
        "\n",
        "def transformed_name(key):\n",
        "  return key + '_xf'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUYeCayFG7kH",
        "colab_type": "text"
      },
      "source": [
        "Now let's define a module containing the `preprocessing_fn()` function that we will pass to the `Transform` component:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uuWiQbOG9ki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_transform_module_file = 'online_news_transform.py'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3EIuVQnBfH7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile {_transform_module_file}\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_transform as tft\n",
        "from online_news_constants import *\n",
        "\n",
        "def preprocessing_fn(inputs):\n",
        "  \"\"\"tf.transform's callback function for preprocessing inputs.\n",
        "\n",
        "  Args:\n",
        "    inputs: map from feature keys to raw not-yet-transformed features.\n",
        "\n",
        "  Returns:\n",
        "    Map from string feature key to transformed feature operations.\n",
        "  \"\"\"\n",
        "  outputs = {}\n",
        "  for key in DENSE_FLOAT_FEATURE_KEYS:\n",
        "    # Preserve this feature as a dense float, setting nan's to the mean.\n",
        "    outputs[transformed_name(key)] = tft.scale_to_z_score(\n",
        "        _fill_in_missing(inputs[key]))\n",
        "\n",
        "  for key in VOCAB_FEATURE_KEYS:\n",
        "    # Build a vocabulary for this feature.\n",
        "    outputs[transformed_name(key)] = tft.compute_and_apply_vocabulary(\n",
        "        _fill_in_missing(inputs[key]),\n",
        "        top_k=VOCAB_SIZE,\n",
        "        num_oov_buckets=OOV_SIZE)\n",
        "\n",
        "  for key in BUCKET_FEATURE_KEYS:\n",
        "    outputs[transformed_name(key)] = tft.bucketize(\n",
        "        _fill_in_missing(inputs[key]), FEATURE_BUCKET_COUNT,\n",
        "        always_return_num_quantiles=False)\n",
        "\n",
        "  for key in CATEGORICAL_FEATURE_KEYS:\n",
        "    outputs[transformed_name(key)] = _fill_in_missing(inputs[key])\n",
        "\n",
        "  # How popular is this article?\n",
        "  outputs[transformed_name(LABEL_KEY)] = _fill_in_missing(inputs[LABEL_KEY])\n",
        "\n",
        "  return outputs\n",
        "\n",
        "def _fill_in_missing(x):\n",
        "  \"\"\"Replace missing values in a SparseTensor.\n",
        "\n",
        "  Fills in missing values of `x` with '' or 0, and converts to a dense tensor.\n",
        "\n",
        "  Args:\n",
        "    x: A `SparseTensor` of rank 2.  Its dense shape should have size at most 1\n",
        "      in the second dimension.\n",
        "\n",
        "  Returns:\n",
        "    A rank 1 tensor where missing values of `x` have been filled in.\n",
        "  \"\"\"\n",
        "  default_value = '' if x.dtype == tf.string else 0\n",
        "  return tf.squeeze(\n",
        "      tf.sparse.to_dense(\n",
        "          tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\n",
        "          default_value),\n",
        "      axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyPi_hd6Q5Ab",
        "colab_type": "text"
      },
      "source": [
        "Now let's run the component"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHfhth_GiZI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Performs transformations and feature engineering in training and serving.\n",
        "transform = Transform(\n",
        "    examples=example_gen.outputs['examples'],\n",
        "    schema=infer_schema.outputs['schema'],\n",
        "    module_file=_transform_module_file)\n",
        "context.run(transform)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jbZO1ykHOeG",
        "colab_type": "text"
      },
      "source": [
        "The `Transform` component has 2 types of outputs:\n",
        "* `transform_graph` is the graph that can perform the preprocessing operations (this graph will be included in the serving and evaluation models).\n",
        "* `transformed_examples` represents the preprocessed training and evaluation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4UjersvAC7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform.outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRFMlRcdHlQy",
        "colab_type": "text"
      },
      "source": [
        "Take a peek at the `transform_graph` artifact: it points to a directory containing 3 subdirectories:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4I-cqfQQvaW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_uri = transform.outputs['transform_graph'].get()[0].uri\n",
        "os.listdir(train_uri)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9374B4RpHzor",
        "colab_type": "text"
      },
      "source": [
        "The `transform_fn` subdirectory contains the actual preprocessing graph. The `metadata` subdirectory contains the schema of the original data. The `transformed_metadata` subdirectory contains the schema of the preprocessed data.\n",
        "\n",
        "Take a look at some of the transformed examples and check that they are indeed processed as intended."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zIepQhSQoPa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_uri = transform.outputs['transformed_examples'].get()[1].uri\n",
        "tfrecord_filenames = [os.path.join(train_uri, name)\n",
        "                      for name in os.listdir(train_uri)]\n",
        "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
        "decoder = tfdv.TFExampleDecoder()\n",
        "for tfrecord in dataset.take(3):\n",
        "  serialized_example = tfrecord.numpy()\n",
        "  example = decoder.decode(serialized_example)\n",
        "  pp.pprint(example)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBJFtnl6lCg9",
        "colab_type": "text"
      },
      "source": [
        "### The Trainer Component\n",
        "\n",
        "The `Trainer` component trains models using TensorFlow.\n",
        "\n",
        "Create a Python module containing a `trainer_fn` function, which must return an estimator.  If you prefer creating a Keras model, you can do so and then convert it to an estimator using `keras.model_to_estimator()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6QNYWc6PD_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setup paths.\n",
        "_trainer_module_file = 'online_news_trainer.py'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaFFTBBeB4wf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile {_trainer_module_file}\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_model_analysis as tfma\n",
        "import tensorflow_transform as tft\n",
        "from tensorflow_transform.tf_metadata import schema_utils\n",
        "\n",
        "from online_news_constants import *\n",
        "\n",
        "\n",
        "def transformed_names(keys):\n",
        "  return [transformed_name(key) for key in keys]\n",
        "\n",
        "\n",
        "# Tf.Transform considers these features as \"raw\"\n",
        "def _get_raw_feature_spec(schema):\n",
        "  return schema_utils.schema_as_feature_spec(schema).feature_spec\n",
        "\n",
        "\n",
        "def _gzip_reader_fn(filenames):\n",
        "  \"\"\"Small utility returning a record reader that can read gzip'ed files.\"\"\"\n",
        "  return tf.data.TFRecordDataset(\n",
        "      filenames,\n",
        "      compression_type='GZIP')\n",
        "\n",
        "\n",
        "def _build_estimator(config, hidden_units=None, warm_start_from=None):\n",
        "  \"\"\"Build an estimator for predicting the popularity of online news articles\n",
        "\n",
        "  Args:\n",
        "    config: tf.estimator.RunConfig defining the runtime environment for the\n",
        "      estimator (including model_dir).\n",
        "    hidden_units: [int], the layer sizes of the DNN (input layer first)\n",
        "    warm_start_from: Optional directory to warm start from.\n",
        "\n",
        "  Returns:\n",
        "    The estimator that will be used for training and eval.\n",
        "  \"\"\"\n",
        "  real_valued_columns = [\n",
        "      tf.feature_column.numeric_column(key, shape=())\n",
        "      for key in transformed_names(DENSE_FLOAT_FEATURE_KEYS)\n",
        "  ]\n",
        "  categorical_columns = [\n",
        "      tf.feature_column.categorical_column_with_identity(\n",
        "          key, num_buckets=VOCAB_SIZE + OOV_SIZE, default_value=0)\n",
        "      for key in transformed_names(VOCAB_FEATURE_KEYS)\n",
        "  ]\n",
        "  categorical_columns += [\n",
        "      tf.feature_column.categorical_column_with_identity(\n",
        "          key, num_buckets=FEATURE_BUCKET_COUNT, default_value=0)\n",
        "      for key in transformed_names(BUCKET_FEATURE_KEYS)\n",
        "  ]\n",
        "  categorical_columns += [\n",
        "      tf.feature_column.categorical_column_with_identity(\n",
        "          key,\n",
        "          num_buckets=num_buckets,\n",
        "          default_value=0) for key, num_buckets in zip(\n",
        "              transformed_names(CATEGORICAL_FEATURE_KEYS),\n",
        "              MAX_CATEGORICAL_FEATURE_VALUES)\n",
        "  ]\n",
        "  return tf.estimator.DNNLinearCombinedRegressor(\n",
        "      config=config,\n",
        "      linear_feature_columns=categorical_columns,\n",
        "      dnn_feature_columns=real_valued_columns,\n",
        "      dnn_hidden_units=hidden_units or [100, 70, 50, 25],\n",
        "      warm_start_from=warm_start_from)\n",
        "\n",
        "\n",
        "def _example_serving_receiver_fn(tf_transform_output, schema):\n",
        "  \"\"\"Build the serving in inputs.\n",
        "\n",
        "  Args:\n",
        "    tf_transform_output: A TFTransformOutput.\n",
        "    schema: the schema of the input data.\n",
        "\n",
        "  Returns:\n",
        "    Tensorflow graph which parses examples, applying tf-transform to them.\n",
        "  \"\"\"\n",
        "  raw_feature_spec = _get_raw_feature_spec(schema)\n",
        "  raw_feature_spec.pop(LABEL_KEY)\n",
        "\n",
        "  raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
        "      raw_feature_spec, default_batch_size=None)\n",
        "  serving_input_receiver = raw_input_fn()\n",
        "\n",
        "  transformed_features = tf_transform_output.transform_raw_features(\n",
        "      serving_input_receiver.features)\n",
        "\n",
        "  return tf.estimator.export.ServingInputReceiver(\n",
        "      transformed_features, serving_input_receiver.receiver_tensors)\n",
        "\n",
        "\n",
        "def _eval_input_receiver_fn(tf_transform_output, schema):\n",
        "  \"\"\"Build everything needed for the tf-model-analysis to run the model.\n",
        "\n",
        "  Args:\n",
        "    tf_transform_output: A TFTransformOutput.\n",
        "    schema: the schema of the input data.\n",
        "\n",
        "  Returns:\n",
        "    EvalInputReceiver function, which contains:\n",
        "      - Tensorflow graph which parses raw untransformed features, applies the\n",
        "        tf-transform preprocessing operators.\n",
        "      - Set of raw, untransformed features.\n",
        "      - Label against which predictions will be compared.\n",
        "  \"\"\"\n",
        "  # Notice that the inputs are raw features, not transformed features here.\n",
        "  raw_feature_spec = _get_raw_feature_spec(schema)\n",
        "\n",
        "  raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
        "      raw_feature_spec, default_batch_size=None)\n",
        "  serving_input_receiver = raw_input_fn()\n",
        "\n",
        "  features = serving_input_receiver.features.copy()\n",
        "  transformed_features = tf_transform_output.transform_raw_features(features)\n",
        "  \n",
        "  # NOTE: Model is driven by transformed features (since training works on the\n",
        "  # materialized output of TFT, but slicing will happen on raw features.\n",
        "  features.update(transformed_features)\n",
        "\n",
        "  return tfma.export.EvalInputReceiver(\n",
        "      features=features,\n",
        "      receiver_tensors=serving_input_receiver.receiver_tensors,\n",
        "      labels=transformed_features[transformed_name(LABEL_KEY)])\n",
        "\n",
        "\n",
        "def _input_fn(filenames, tf_transform_output, batch_size=200):\n",
        "  \"\"\"Generates features and labels for training or evaluation.\n",
        "\n",
        "  Args:\n",
        "    filenames: [str] list of CSV files to read data from.\n",
        "    tf_transform_output: A TFTransformOutput.\n",
        "    batch_size: int First dimension size of the Tensors returned by input_fn\n",
        "\n",
        "  Returns:\n",
        "    A (features, indices) tuple where features is a dictionary of\n",
        "      Tensors, and indices is a single Tensor of label indices.\n",
        "  \"\"\"\n",
        "  transformed_feature_spec = (\n",
        "      tf_transform_output.transformed_feature_spec().copy())\n",
        "\n",
        "  dataset = tf.data.experimental.make_batched_features_dataset(\n",
        "      filenames, batch_size, transformed_feature_spec, reader=_gzip_reader_fn)\n",
        "\n",
        "  transformed_features = dataset.make_one_shot_iterator().get_next()\n",
        "  # We pop the label because we do not want to use it as a feature while we're\n",
        "  # training.\n",
        "  return transformed_features, transformed_features.pop(\n",
        "      transformed_name(LABEL_KEY))\n",
        "\n",
        "\n",
        "# TFX will call this function\n",
        "def trainer_fn(hparams, schema):\n",
        "  \"\"\"Build the estimator using the high level API.\n",
        "  Args:\n",
        "    hparams: Holds hyperparameters used to train the model as name/value pairs.\n",
        "    schema: Holds the schema of the training examples.\n",
        "  Returns:\n",
        "    A dict of the following:\n",
        "      - estimator: The estimator that will be used for training and eval.\n",
        "      - train_spec: Spec for training.\n",
        "      - eval_spec: Spec for eval.\n",
        "      - eval_input_receiver_fn: Input function for eval.\n",
        "  \"\"\"\n",
        "  # Number of nodes in the first layer of the DNN\n",
        "  first_dnn_layer_size = 100\n",
        "  num_dnn_layers = 4\n",
        "  dnn_decay_factor = 0.7\n",
        "\n",
        "  train_batch_size = 40\n",
        "  eval_batch_size = 40\n",
        "\n",
        "  tf_transform_output = tft.TFTransformOutput(hparams.transform_output)\n",
        "\n",
        "  train_input_fn = lambda: _input_fn(\n",
        "      hparams.train_files,\n",
        "      tf_transform_output,\n",
        "      batch_size=train_batch_size)\n",
        "\n",
        "  eval_input_fn = lambda: _input_fn(\n",
        "      hparams.eval_files,\n",
        "      tf_transform_output,\n",
        "      batch_size=eval_batch_size)\n",
        "\n",
        "  train_spec = tf.estimator.TrainSpec(\n",
        "      train_input_fn,\n",
        "      max_steps=hparams.train_steps)\n",
        "\n",
        "  serving_receiver_fn = lambda: _example_serving_receiver_fn(\n",
        "      tf_transform_output, schema)\n",
        "\n",
        "  exporter = tf.estimator.FinalExporter('online-news', serving_receiver_fn)\n",
        "  eval_spec = tf.estimator.EvalSpec(\n",
        "      eval_input_fn,\n",
        "      steps=hparams.eval_steps,\n",
        "      exporters=[exporter],\n",
        "      name='online-news-eval')\n",
        "\n",
        "  run_config = tf.estimator.RunConfig(\n",
        "      save_checkpoints_steps=999, keep_checkpoint_max=1)\n",
        "\n",
        "  run_config = run_config.replace(model_dir=hparams.serving_model_dir)\n",
        "\n",
        "  estimator = _build_estimator(\n",
        "      # Construct layers sizes with exponetial decay\n",
        "      hidden_units=[\n",
        "          max(2, int(first_dnn_layer_size * dnn_decay_factor**i))\n",
        "          for i in range(num_dnn_layers)\n",
        "      ],\n",
        "      config=run_config,\n",
        "      warm_start_from=hparams.warm_start_from)\n",
        "\n",
        "  # Create an input receiver for TFMA processing\n",
        "  receiver_fn = lambda: _eval_input_receiver_fn(\n",
        "      tf_transform_output, schema)\n",
        "\n",
        "  return {\n",
        "      'estimator': estimator,\n",
        "      'train_spec': train_spec,\n",
        "      'eval_spec': eval_spec,\n",
        "      'eval_input_receiver_fn': receiver_fn\n",
        "  }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnLjStUJIoos",
        "colab_type": "text"
      },
      "source": [
        "Create and run the `Trainer` component, passing it the file that we created above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "429-vvCWibO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Uses user-provided Python function that implements a model using TensorFlow's\n",
        "# Estimators API.\n",
        "trainer = Trainer(\n",
        "    module_file=_trainer_module_file,\n",
        "    transformed_examples=transform.outputs['transformed_examples'],\n",
        "    schema=infer_schema.outputs['schema'],\n",
        "    transform_graph=transform.outputs['transform_graph'],\n",
        "    train_args=trainer_pb2.TrainArgs(num_steps=10000),\n",
        "    eval_args=trainer_pb2.EvalArgs(num_steps=5000))\n",
        "context.run(trainer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktJA8On9Iui7",
        "colab_type": "text"
      },
      "source": [
        "Take a peek at the trained model which was exported from `Trainer`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDBZG9Oso-BD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_uri = trainer.outputs['model'].get()[0].uri\n",
        "serving_model_path = os.path.join(train_uri, 'serving_model_dir', 'export', 'online-news')\n",
        "latest_serving_model_path = os.path.join(serving_model_path, max(os.listdir(serving_model_path)))\n",
        "exported_model = tf.saved_model.load(latest_serving_model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyT3ZVGCZWsj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "exported_model.graph.get_operations()[:10] + [\"...\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpF7caML7WLB",
        "colab_type": "text"
      },
      "source": [
        "## Analyze Training with TensorBoard\n",
        "\n",
        "Use [TensorBoard](https://www.tensorflow.org/tensorboard) to analyze the model training that was done in Trainer, and see how well our model trained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjCXDSnX7mjQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGcJtyH87m68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir {os.path.join(train_uri, 'serving_model_dir')}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmPftrv0lEQy",
        "colab_type": "text"
      },
      "source": [
        "### The Evaluator Component\n",
        "\n",
        "The `Evaluator` component analyzes model performance using the TensorFlow Model Analysis library.  It runs inference requests on particular subsets of the test dataset, based on which `slices` are defined by the developer.  Knowing which slices should be analyzed requires domain knowledge of what is important in this particular use case or domain.  The slice chosen for this example is `weekday`.\n",
        "\n",
        "Create and run an Evaluator component."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKSgnlNyRCIg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_analyzer = Evaluator(\n",
        "    examples=example_gen.outputs['examples'],\n",
        "    model=trainer.outputs['model'],\n",
        ")\n",
        "context.run(model_analyzer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-f1ZyycwUnLr",
        "colab_type": "text"
      },
      "source": [
        "Let's load the `Evaluator` results and render them using the `tfma.view.render_slicing_metrics()` function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0xdNnlHUkXQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluation_uri = model_analyzer.outputs['output'].get()[0].uri\n",
        "eval_result = tfma.load_eval_result(evaluation_uri)\n",
        "tfma.view.render_slicing_metrics(eval_result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_USIGaiRPAj",
        "colab_type": "text"
      },
      "source": [
        "We can also pass feature slice specifications if we want to evaluate the quality of the model over specific subsets of the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zjcx8g6mihSt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Uses TFMA to compute a evaluation statistics over features of a model.\n",
        "model_analyzer = Evaluator(\n",
        "    examples=example_gen.outputs['examples'],\n",
        "    model=trainer.outputs['model'],\n",
        "    feature_slicing_spec=evaluator_pb2.FeatureSlicingSpec(specs=[\n",
        "        evaluator_pb2.SingleSlicingSpec(\n",
        "            column_for_slicing=['weekday'])\n",
        "    ]))\n",
        "context.run(model_analyzer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-E2b7aroU4DH"
      },
      "source": [
        "Let's look at the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J_YkjWCBU4DJ",
        "colab": {}
      },
      "source": [
        "evaluation_uri = model_analyzer.outputs['output'].get()[0].uri\n",
        "eval_result = tfma.load_eval_result(evaluation_uri)\n",
        "tfma.view.render_slicing_metrics(\n",
        "      eval_result,\n",
        "      slicing_spec=tfma.slicer.SingleSliceSpec(columns=['weekday']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_3Q8oi7W80D",
        "colab_type": "text"
      },
      "source": [
        "The metrics are also accessible programmatically:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ya3NKcxO8vjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for metric in eval_result.slicing_metrics:\n",
        "  pp.pprint(metric)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76Mil-7FlF_y",
        "colab_type": "text"
      },
      "source": [
        "### The ModelValidator Component\n",
        "\n",
        "The `ModelValidator` component performs validation of your candidate model compared to the previously deployed model (if any) using criteria that you define, or to a baseline value.  If the new model scores better than the previous model it will be \"blessed\" by ModelValidator, approving it for deployment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXk1MA7sijCr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Performs quality validation of a candidate model (compared to a baseline).\n",
        "model_validator = ModelValidator(\n",
        "    examples=example_gen.outputs['examples'],\n",
        "    model=trainer.outputs['model'])\n",
        "context.run(model_validator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVOkA2MA9FtN",
        "colab_type": "text"
      },
      "source": [
        "Examine the output of ModelValidator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-si25tpAQ0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_validator.outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2RWZWD-AZ-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "blessing_uri = model_validator.outputs.blessing.get()[0].uri\n",
        "!ls -l {blessing_uri}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8DYekCZlHfj",
        "colab_type": "text"
      },
      "source": [
        "### The Pusher Component\n",
        "\n",
        "The `Pusher` component checks whether a model has been \"blessed\", and if so, deploys it to production by pushing the model to a well known file destination."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvVasBxePW-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setup serving path\n",
        "_serving_model_dir = os.path.join(tempfile.mkdtemp(),\n",
        "                                  'serving_model/online_news_simple')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nDI_54N9Sbk",
        "colab_type": "text"
      },
      "source": [
        "Create and run a Pusher component."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r45nQ69eikc9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Checks whether the model passed the validation steps and pushes the model\n",
        "# to a file destination if check passed.\n",
        "pusher = Pusher(\n",
        "    model=trainer.outputs['model'],\n",
        "    model_blessing=model_validator.outputs['blessing'],\n",
        "    push_destination=pusher_pb2.PushDestination(\n",
        "        filesystem=pusher_pb2.PushDestination.Filesystem(\n",
        "            base_directory=_serving_model_dir)))\n",
        "context.run(pusher)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fj6imIJx9YGD",
        "colab_type": "text"
      },
      "source": [
        "Examine the output of Pusher."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNvMj9AWsmSt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pusher.outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bphCjS-B4vv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "push_uri = pusher.outputs['pushed_model'].get()[0].uri\n",
        "latest_version = max(os.listdir(push_uri))\n",
        "latest_version_path = os.path.join(push_uri, latest_version)\n",
        "model = tf.saved_model.load(latest_version_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7yifs_19iQG",
        "colab_type": "text"
      },
      "source": [
        "Review the model signatures and methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQAmjZ81B8xm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for item in model.signatures.items():\n",
        "  pp.pprint(item)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5cKQjPJX0IW",
        "colab_type": "text"
      },
      "source": [
        "Alternartively, we can use the command line utility `saved_model_cli` to look at the [MetaGraphDefs](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/MetaGraphDef) (the models) and [SignatureDefs](../signature_defs) (the methods you can call) in our SavedModel.  See [this discussion of the SavedModel CLI](https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/saved_model.md#cli-to-inspect-and-execute-savedmodel) in the TensorFlow Guide."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3YIGoRuX0-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "latest_pushed_model = os.path.join(_serving_model_dir, max(os.listdir(_serving_model_dir)))\n",
        "!saved_model_cli show --dir {latest_pushed_model} --all"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2x8jdexxO9y",
        "colab_type": "text"
      },
      "source": [
        "That tells us a few important things about our model.  In this case we just trained our model, so we already know the inputs and outputs, but if we didn't this would be important information.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1USTBiz7xMC0",
        "colab_type": "text"
      },
      "source": [
        "##TensorFlow Serving\n",
        "\n",
        "Now that we have a trained model that has been blessed by ModelValidator, and pushed to our deployment target by Pusher, we can load it into TensorFlow Serving and start serving inference requests."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxikHooyYBjM",
        "colab_type": "text"
      },
      "source": [
        "### Add TensorFlow Serving distribution URI as a package source\n",
        "\n",
        "We're preparing to install TensorFlow Serving using [Aptitude](https://wiki.debian.org/Aptitude) since this Colab runs in a Debian environment.  We'll add the `tensorflow-model-server` package to the list of packages that Aptitude knows about.  Note that we're running as root.\n",
        "\n",
        "Note: This example is running TensorFlow Serving natively, but [you can also run it in a Docker container](https://www.tensorflow.org/tfx/serving/docker), which is one of the easiest ways to get started using TensorFlow Serving."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG_juAPUYCbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is the same as you would do from your command line, but without the [arch=amd64], and no sudo\n",
        "# You would instead do:\n",
        "# echo \"deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | sudo tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\n",
        "# curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | sudo apt-key add -\n",
        "\n",
        "!echo \"deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\n",
        "curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | apt-key add -\n",
        "!apt update"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAw4M8jIYRLz",
        "colab_type": "text"
      },
      "source": [
        "### Install TensorFlow Serving\n",
        "\n",
        "This is all you need - one command line!  Please note that running TensorFlow Serving in a Docker Container is also a great option, with a lot of advantages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5D9eXsAYVMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install tensorflow-model-server"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBtbWxkEYdaj",
        "colab_type": "text"
      },
      "source": [
        "### Start running TensorFlow Serving\n",
        "\n",
        "This is where we start running TensorFlow Serving and load our model.  After it loads we can start making inference requests using REST.  There are some important parameters:\n",
        "\n",
        "* `rest_api_port`: The port that you'll use for REST requests.\n",
        "* `model_name`: You'll use this in the URL of REST requests.  It can be anything.\n",
        "* `model_base_path`: This is the path to the directory where you've saved your model. Note that this base_path should _not_ include the model version directory, which is why we split it off below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_Pe6MtMYh1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.environ[\"MODEL_DIR\"] = os.path.split(latest_pushed_model)[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k31wLFSVYl-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash --bg \n",
        "nohup tensorflow_model_server \\\n",
        "  --rest_api_port=8501 \\\n",
        "  --model_name=online_news_simple \\\n",
        "  --model_base_path=\"${MODEL_DIR}\" >server.log 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O42M65ArYo2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tail server.log"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPBt40HnY9rI",
        "colab_type": "text"
      },
      "source": [
        "### Perform Inference on example data\n",
        "\n",
        "Let's load some examples from the eval dataset, remove their labels (as the serving model does not expect labels) and send them to Tensorflow Serving through a single REST API call. Note that this will include the labels, but the server will ignore them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xfa5V4nva0Dn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_uri = example_gen.outputs['examples'].get()[1].uri\n",
        "eval_tfrecord_paths = [os.path.join(eval_uri, name)\n",
        "                      for name in os.listdir(eval_uri)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOJ5DN7E505f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def strip_label(serialized_example):\n",
        "  example = tf.train.Example.FromString(serialized_example.numpy())\n",
        "  del example.features.feature[\"n_shares_percentile\"]\n",
        "  return example.SerializeToString()\n",
        "\n",
        "dataset = tf.data.TFRecordDataset(eval_tfrecord_paths,\n",
        "                                  compression_type=\"GZIP\")\n",
        "serialized_examples = [strip_label(serialized_example)\n",
        "                       for serialized_example in dataset.take(3)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Xs5x8S1Y-Yd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def do_inference(server_addr, model_name, serialized_examples):\n",
        "  \"\"\"Sends requests to the model and prints the results.\n",
        "  Args:\n",
        "    server_addr: network address of model server in \"host:port\" format\n",
        "    model_name: name of the model as understood by the model server\n",
        "    serialized_examples: serialized examples of data to do inference on\n",
        "  \"\"\"\n",
        "  parsed_server_addr = server_addr.split(':')\n",
        "\n",
        "  host=parsed_server_addr[0]\n",
        "  port=parsed_server_addr[1]\n",
        "  json_examples = []\n",
        "  \n",
        "  for serialized_example in serialized_examples:\n",
        "    # The encoding follows the guidelines in:\n",
        "    # https://www.tensorflow.org/tfx/serving/api_rest\n",
        "    example_bytes = base64.b64encode(serialized_example).decode('utf-8')\n",
        "    predict_request = '{ \"b64\": \"%s\" }' % example_bytes\n",
        "    json_examples.append(predict_request)\n",
        "\n",
        "  json_request = '{ \"instances\": [' + ','.join(map(str, json_examples)) + ']}'\n",
        "\n",
        "  server_url = 'http://' + host + ':' + port + '/v1/models/' + model_name + ':predict'\n",
        "  response = requests.post(\n",
        "      server_url, data=json_request, timeout=5.0)\n",
        "  response.raise_for_status()\n",
        "  prediction = response.json()\n",
        "  print(json.dumps(prediction, indent=4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOs21jlaZGWi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "do_inference(server_addr='127.0.0.1:8501', \n",
        "     model_name='online_news_simple',\n",
        "     serialized_examples=serialized_examples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmT39IX-9rkb",
        "colab_type": "text"
      },
      "source": [
        "### Pipeline Complete!\n",
        "\n",
        "In this example you created a TFX pipeline in a Colab notebook, using the InteractiveContext.  Along the way you learned about each of the standard TFX components, but if the standard components don't meet all of your needs you can create your own custom components!  Custom components will be covered in a later lesson."
      ]
    }
  ]
}
